<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="author" content="Alex Ioannides">
  <title>Transformers &amp; LLMs</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="https://unpkg.com/reveal.js@^4//dist/reset.css">
  <link rel="stylesheet" href="https://unpkg.com/reveal.js@^4//dist/reveal.css">
  <style>
    .reveal .sourceCode {  /* see #7635 */
      overflow: visible;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        background-color: #232629;
        color: #7a7c7d;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #7a7c7d;  padding-left: 4px; }
    div.sourceCode
      { color: #cfcfc2; background-color: #232629; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #cfcfc2; } /* Normal */
    code span.al { color: #95da4c; background-color: #4d1f24; font-weight: bold; } /* Alert */
    code span.an { color: #3f8058; } /* Annotation */
    code span.at { color: #2980b9; } /* Attribute */
    code span.bn { color: #f67400; } /* BaseN */
    code span.bu { color: #7f8c8d; } /* BuiltIn */
    code span.cf { color: #fdbc4b; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #3daee9; } /* Char */
    code span.cn { color: #27aeae; font-weight: bold; } /* Constant */
    code span.co { color: #7a7c7d; } /* Comment */
    code span.cv { color: #7f8c8d; } /* CommentVar */
    code span.do { color: #a43340; } /* Documentation */
    code span.dt { color: #2980b9; } /* DataType */
    code span.dv { color: #f67400; } /* DecVal */
    code span.er { color: #da4453; text-decoration: underline; } /* Error */
    code span.ex { color: #0099ff; font-weight: bold; } /* Extension */
    code span.fl { color: #f67400; } /* Float */
    code span.fu { color: #8e44ad; } /* Function */
    code span.im { color: #27ae60; } /* Import */
    code span.in { color: #c45b00; } /* Information */
    code span.kw { color: #cfcfc2; font-weight: bold; } /* Keyword */
    code span.op { color: #cfcfc2; } /* Operator */
    code span.ot { color: #27ae60; } /* Other */
    code span.pp { color: #27ae60; } /* Preprocessor */
    code span.re { color: #2980b9; background-color: #153042; } /* RegionMarker */
    code span.sc { color: #3daee9; } /* SpecialChar */
    code span.ss { color: #da4453; } /* SpecialString */
    code span.st { color: #f44f4f; } /* String */
    code span.va { color: #27aeae; } /* Variable */
    code span.vs { color: #da4453; } /* VerbatimString */
    code span.wa { color: #da4453; } /* Warning */
  </style>
  <link rel="stylesheet" href="https://unpkg.com/reveal.js@^4//dist/theme/solarized.css" id="theme">
  <link rel="stylesheet" href="custom.css"/>
</head>
<body>
  <div class="reveal">
    <div class="slides">

<section id="title-slide">
  <h1 class="title">Transformers &amp; LLMs</h1>
  <p class="author">Alex Ioannides</p>
  <p class="date">October 12th, 2023</p>
</section>

<section>
<section
id="the-story-of-one-mans-mission-not-to-get-left-behind-in-the-dust"
class="title-slide slide level2">
<h2>The story of one man’s mission not to get left behind in the
dust</h2>

</section>
<section class="slide level3">

<p>This presentation is based on the codebase at <a
href="https://github.com/AlexIoannides/transformers-gen-ai">github.com/AlexIoannides/transformer-gen-ai</a>.</p>
<p>I’m not going to assume you’ve worked through it, but if you have and
there are questions you want to ask, then please do 🙂</p>
</section>
<section class="slide level3">

<p>The repo contains the source code for a Python package called
<code>modelling</code>. This implements a generative transformer model
and the tools to use it. Examples are contained in a series of
notebooks.</p>
<p><img data-src="images/repo_layout.svg" style="width:60.0%" /></p>
</section>
<section class="slide level3">

<p>The aim was to develop a platform for understanding how transformer
models work, together with the (ML) engineering challenges that they
pose.</p>
</section>
<section class="slide level3">

<p>This project made heavy use of the <a
href="https://pytorch.org/docs/stable/index.html">PyTorch</a> tensor
computation framework and its ecosystem. If you want to learn how to use
this, try starting with the <a
href="https://alexioannides.com/notes-and-demos/pytorch/">introduction
on my personal website</a></p>
<p><img data-src="images/intro_to_pytorch.png"
style="width:50.0%" /></p>
</section></section>
<section id="what-im-intending-to-talk-about"
class="title-slide slide level2">
<h2>What I’m intending to talk about</h2>
<div>
<ol type="1">
<li class="fragment">The problem we’re trying to solve</li>
<li class="fragment">How to compute multi-head attention.</li>
<li class="fragment">Transformers: encoders, decoders, and all
that.</li>
<li class="fragment">How I developed a generative LLM.</li>
<li class="fragment">Exciting things to try with this LLM.</li>
</ol>
</div>
</section>

<section>
<section id="the-problem-were-trying-to-solve"
class="title-slide slide level2">
<h2>The problem we’re trying to solve</h2>

</section>
<section class="slide level3">

<p>The state of applied NLP in 2023 (according to Alex)</p>
<p><img data-src="images/paradigm.png" style="width:60.0%" /></p>
</section>
<section class="slide level3">

<p>The role that attention plays in all this…</p>
<p><img data-src="images/attention.png" /></p>
</section></section>
<section>
<section id="how-to-compute-multi-head-attention"
class="title-slide slide level2">
<h2>How to compute multi-head attention</h2>

</section>
<section id="lets-start-with-self-attention" class="slide level3">
<h3>Let’s start with self-attention</h3>
</section>
<section class="slide level3">

<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>VOCAB_SIZE <span class="op">=</span> <span class="dv">20000</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>EMBEDDING_DIM <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Let&#39;s assume some tokenizer has tokenised our sentence.</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>tokenized_sentence <span class="op">=</span> torch.randint(<span class="dv">0</span>, vocab_size, <span class="dv">8</span>)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>n_tokens <span class="op">=</span> <span class="bu">len</span>(tokenized_sentence)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="co"># We then map from token to embeddings.</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>embedding_layer <span class="op">=</span> nn.Embedding(VOCAB_SIZE, EMBEDDING_DIM)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>embedded_tokens <span class="op">=</span> embedding_layer(tokenized_sentence)</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="co"># And compute self-attention weights.</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>attn_weights <span class="op">=</span> torch.empty(n_tokens, n_tokens)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_tokens):</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(n_tokens):</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>        attn_weights[i, j] <span class="op">=</span> torch.dot(embedded_tokens[i], embedded_tokens[j])</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Normalise the weights, so that they sun to 1.0.</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>attn_weights_norm <span class="op">=</span> F.softmax(attn_weights <span class="op">/</span> math.sqrt(EMBEDDING_DIM), dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a><span class="co"># And finally use the weights to compute context-aware embeddings.</span></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>context_weighted_embeddings <span class="op">=</span> torch.matmul(attn_weights_norm, embedded_tokens)</span></code></pre></div>
</section>
<section class="slide level3">

<p>More formally…</p>
<p><span class="math display">\[
\vec{x_{i}} \to \vec{z_{i}} = \sum_{j=1}^{N}{a_{ij} \times \vec{x_{j}}}
\]</span></p>
<p>i.e., we build new embeddings using semantic similarity to
selectively pool information from the original embeddings. Note, there
aren’t any attention-specific parameters that need to be learnt, only
the original embeddings. We’ll come back to this later.</p>
</section>
<section id="time-and-causality" class="slide level3">
<h3>Time and causality</h3>
<p>In the current setup, the attention-modulated embedding at time <span
class="math inline">\(t_1\)</span> is a function of embeddings for
tokens that come after. If we want to develop generative models, then
this isn’t appropriate. A common solution is to use <strong>causal
masking</strong>.</p>
</section>
<section class="slide level3">

<p><img data-src="images/causal_mask.png" /></p>
<div class="sourceCode" id="cb2"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>causal_mask <span class="op">=</span> torch.triu(torch.full((n_tokens, n_tokens), <span class="va">True</span>), diagonal<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>causal_attn_weights <span class="op">=</span> attn_weights.masked_fill(causal_mask, <span class="op">-</span><span class="fl">1e10</span>)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>causal_attn_weights_norm <span class="op">=</span> F.softmax(causal_attn_weights <span class="op">/</span> math.sqrt(EMBEDDING_DIM), dim<span class="op">=</span><span class="dv">1</span>)</span></code></pre></div>
</section>
<section id="learning-how-to-attend" class="slide level3">
<h3>Learning how to attend</h3>
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define three linear transformations.</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>u_q <span class="op">=</span> torch.rand(n_tokens, n_tokens)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>u_k <span class="op">=</span> torch.rand(n_tokens, n_tokens)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>u_v <span class="op">=</span> torch.rand(n_tokens, n_tokens)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Use these to transform the embedded tokens.</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>q <span class="op">=</span> torch.matmul(u_q, embedded_tokens)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>k <span class="op">=</span> torch.matmul(u_k, embedded_tokens)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>v <span class="op">=</span> torch.matmul(u_v, embedded_tokens)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="co"># And then re-work the computation of the attention weights.</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>attn_weights_param <span class="op">=</span> torch.empty(n_tokens, n_tokens)</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_tokens):</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(n_tokens):</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>        attn_weights_param[i, j] <span class="op">=</span> torch.dot(q[i], k[j])</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>attn_weights_param_norm <span class="op">=</span> F.softmax(</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>    attn_weights_param <span class="op">/</span> math.sqrt(EMBEDDING_DIM), dim<span class="op">=</span><span class="dv">1</span></span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>context_weighted_embeddings_param <span class="op">=</span> torch.matmul(attn_weights_param_norm, v)</span></code></pre></div>
<p>This is equivalent to passing <code>embedded_tokens</code> through
three separate linear network layers and using the outputs within the
self-attention mechanism.</p>
</section>
<section id="from-single-to-multiple-attention-heads"
class="slide level3">
<h3>From single to multiple attention heads</h3>
<p><img data-src="images/multi_head_attention.png" /></p>
</section>
<section class="slide level3">

<p>“<em>The ‘Attention is all you need’ paper was written at a time when
the idea of factoring feature spaces into independent subspaces had been
shown to provide great benefits for computer vision models… Multi-head
attention is simply the application of the same idea to
self-attention.</em>”</p>
<p>- François Chollet (the author of Keras)</p>
</section>
<section class="slide level3">

<p>We have now arrived at</p>
<p><code>torch.nn.MultiheadAttention</code></p>
</section></section>
<section>
<section id="transformers-encoders-decoders-and-all-that"
class="title-slide slide level2">
<h2>Transformers: encoders, decoders, and all that</h2>

</section>
<section class="slide level3">

<p>How do we arrive at</p>
<p><code>torch.nn.TransformerEncoderLayer</code>
<code>torch.nn.TransformerDecoderLayer</code></p>
<p>?</p>
</section>
<section class="slide level3">

<p>Encoder-decoder for seq-to-seq translation</p>
<p><img data-src="images/encoder_decoder.png" style="width:40.0%" /></p>
<p>- “<em>Attention is all you Need</em>”, Vaswani et al. (2017)</p>
</section>
<section class="slide level3">

<p>“<em>… adding residual connections, adding normalization layers—all
of these are standard architecture patterns that one would be wise to
leverage in any complex model. Together, these bells and whistles form
the Transformer encoder—one of two critical parts that make up the
Transformer architecture</em>”</p>
<p>- François Chollet (the author of Keras)</p>
</section>
<section class="slide level3">

<p>“<em>We stare into the void where our math fails us and try to write
math papers anyway… We could then turn to the deepness itself and prove
things about batch norm or dropout or whatever, but these just give us
some nonpredictive post hoc justifications… deep learning seems to drive
people completely insane.</em>”</p>
<p>- <a href="https://argmin.substack.com/p/my-mathematical-mind">Ben
Recht</a> (Prof. of Computer Sciences, UC Berkley)</p>
</section>
<section id="when-do-we-use-encoders-decoders-or-both"
class="slide level3">
<h3>When do we use encoders, decoders, or both?</h3>
<div>
<ul>
<li class="fragment"><strong>Encoder</strong>: pure embedding
models.</li>
<li class="fragment"><strong>Decoder</strong>: generative models.</li>
<li class="fragment"><strong>Encoder + Decoder</strong>:
sequence-to-sequence models.</li>
</ul>
</div>
</section></section>
<section>
<section id="how-i-developed-a-generative-llm"
class="title-slide slide level2">
<h2>How I developed a generative LLM</h2>

</section>
<section id="the-data" class="slide level3">
<h3>The data</h3>
<p>50k film reviews and sentiment scores from IMDB.</p>
<div class="sourceCode" id="cb4"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.nn.utils.rnn <span class="im">import</span> pad_sequence</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader, IterableDataset</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchtext.datasets <span class="im">import</span> IMDB</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchtext.vocab <span class="im">import</span> vocab</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> modelling.data <span class="im">import</span> (</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    FilmReviewSequences,</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    IMDBTokenizer,</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    get_data,</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>    make_chunks,</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>    make_sequence_datasets,</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>    pad_seq2seq_data,</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> get_data()</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>data.head(<span class="dv">10</span>)</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a><span class="co"># sentiment review</span></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a><span class="co"># 0 0   Forget what I said about Emeril. Rachael Ray i...</span></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a><span class="co"># 1 0   Former private eye-turned-security guard ditch...</span></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a><span class="co"># 2 0   Mann photographs the Alberta Rocky Mountains i...</span></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a><span class="co"># 3 0   Simply put: the movie is boring. Cliché upon c...</span></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a><span class="co"># 4 1   Now being a fan of sci fi, the trailer for thi...</span></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a><span class="co"># 5 1   In &#39;Hoot&#39; Logan Lerman plays Roy Eberhardt, th...</span></span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a><span class="co"># 6 0   This is the worst film I have ever seen.I was ...</span></span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a><span class="co"># 7 1   I think that Toy Soldiers is an excellent movi...</span></span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a><span class="co"># 8 0   I think Micheal Ironsides acting career must b...</span></span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a><span class="co"># 9 0   This was a disgrace to the game FarCry i had m...</span></span></code></pre></div>
</section>
<section class="slide level3">

<p>Example #4 in full:</p>
<p><em>“Now being a fan of sci fi, the trailer for this film looked a
bit too, how do i put it, hollywood. But after watching it i can gladly
say it has impressed me greatly. Jude is a class actor and miss Leigh
pulls it off better than she did in Delores Clairborne. It brings films
like The Matrix, 12 Monkeys and The Cell into mind, which might not
sound that appealing, but it truly is one of the best films i have
seen.”</em></p>
</section>
<section id="chunking" class="slide level3">
<h3>Chunking</h3>
<p>Most reviews are too long to be used as one input sequence and need
to be broken into chunks. I chose a strategy based on preserving
sentence integrity to create overlapping chunks that fall within a
maximum sequence length.</p>
</section>
<section class="slide level3">

<p>Example with maximum sequence length of 30 words:</p>
<div class="sourceCode" id="cb5"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>full_text <span class="op">=</span> <span class="st">&quot;&quot;&quot;I&#39;ve seen things you people wouldn&#39;t believe. Attack ships on fire off</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="st">the shoulder of Orion. I watched C-beams glitter in the dark near the Tannhäuser Gate.</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="st">All those moments will be lost in time, like tears in rain.&quot;&quot;&quot;</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>chunk_one <span class="op">=</span> <span class="st">&quot;&quot;&quot;I&#39;ve seen things you people wouldn&#39;t believe. Attack ships on fire off</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="st">the shoulder of Orion. I watched C-beams glitter in the dark near the Tannhäuser Gate.&quot;&quot;&quot;</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>chunk_three <span class="op">=</span> <span class="st">&quot;&quot;&quot;Attack ships on fire off the shoulder of Orion. I watched C-beams</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="st">glitter in the dark near the Tannhäuser Gate.&quot;&quot;&quot;</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>chunk_four <span class="op">=</span> <span class="st">&quot;&quot;&quot;I watched C-beams glitter in the dark near the Tannhäuser Gate. All</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a><span class="st">those moments will be lost in time, like tears in rain.&quot;&quot;&quot;</span></span></code></pre></div>
</section>
<section id="generating-tokens" class="slide level3">
<h3>Generating tokens</h3>
<div class="sourceCode" id="cb6"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> IMDBTokenizer(_Tokenizer):</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Word to integer tokenization for use with any dataset or model.&quot;&quot;&quot;</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, reviews: <span class="bu">list</span>[<span class="bu">str</span>], min_word_freq: <span class="bu">int</span> <span class="op">=</span> <span class="dv">1</span>):</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>        reviews_doc <span class="op">=</span> <span class="st">&quot; &quot;</span>.join(reviews)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>        token_counter <span class="op">=</span> Counter(<span class="va">self</span>._tokenize(reviews_doc))</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>        token_freqs <span class="op">=</span> <span class="bu">sorted</span>(token_counter.items(), key<span class="op">=</span><span class="kw">lambda</span> e: e[<span class="dv">1</span>], reverse<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>        _vocab <span class="op">=</span> vocab(OrderedDict(token_freqs), min_freq<span class="op">=</span>min_word_freq)</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>        _vocab.insert_token(<span class="st">&quot;&lt;pad&gt;&quot;</span>, PAD_TOKEN_IDX)</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>        _vocab.insert_token(<span class="st">&quot;&lt;unk&gt;&quot;</span>, UNKOWN_TOKEN_IDX)</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>        _vocab.set_default_index(<span class="dv">1</span>)</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.vocab <span class="op">=</span> _vocab</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.vocab_size <span class="op">=</span> <span class="bu">len</span>(<span class="va">self</span>.vocab)</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> text2tokens(<span class="va">self</span>, text: <span class="bu">str</span>) <span class="op">-&gt;</span> <span class="bu">list</span>[<span class="bu">int</span>]:</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.vocab(<span class="va">self</span>._tokenize(text))</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> tokens2text(<span class="va">self</span>, tokens: <span class="bu">list</span>[<span class="bu">int</span>]) <span class="op">-&gt;</span> <span class="bu">str</span>:</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>        text <span class="op">=</span> <span class="st">&quot; &quot;</span>.join(<span class="va">self</span>.vocab.lookup_tokens(tokens))</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>        text <span class="op">=</span> re.sub(<span class="vs">rf&quot;\s</span><span class="sc">{</span>EOS_TOKEN<span class="sc">}</span><span class="vs">&quot;</span>, <span class="st">&quot;.&quot;</span>, text)</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> text.strip()</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>    <span class="at">@staticmethod</span></span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _tokenize(text: <span class="bu">str</span>) <span class="op">-&gt;</span> <span class="bu">list</span>[<span class="bu">str</span>]:</span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>        text <span class="op">=</span> IMDBTokenizer._standardise(text)</span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>        text <span class="op">=</span> (<span class="st">&quot;. &quot;</span>.join(sentence.strip() <span class="cf">for</span> sentence <span class="kw">in</span> text.split(<span class="st">&quot;.&quot;</span>))).strip()</span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a>        text <span class="op">=</span> re.sub(<span class="vs">r&quot;\.&quot;</span>, <span class="ss">f&quot; </span><span class="sc">{</span>EOS_TOKEN<span class="sc">}</span><span class="ss"> &quot;</span>, text)</span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a>        text <span class="op">=</span> re.sub(<span class="vs">r&quot;\s+&quot;</span>, <span class="st">&quot; &quot;</span>, text)</span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> text.split()</span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a>    ...</span></code></pre></div>
</section>
<section class="slide level3">

<div class="sourceCode" id="cb7"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> IMDBTokenizer(_Tokenizer):</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Word to integer tokenization for use with any dataset or model.&quot;&quot;&quot;</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    ...</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">@staticmethod</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _standardise(text: <span class="bu">str</span>) <span class="op">-&gt;</span> <span class="bu">str</span>:</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>        <span class="co">&quot;&quot;&quot;Remove punctuation, HTML and make lower case.&quot;&quot;&quot;</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>        text <span class="op">=</span> text.lower().strip()</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>        text <span class="op">=</span> unidecode(text)</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>        text <span class="op">=</span> re.sub(<span class="vs">r&quot;&lt;[^&gt;]*&gt;&quot;</span>, <span class="st">&quot;&quot;</span>, text)</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>        text <span class="op">=</span> re.sub(<span class="vs">r&quot;mr.&quot;</span>, <span class="st">&quot;mr&quot;</span>, text)</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>        text <span class="op">=</span> re.sub(<span class="vs">r&quot;mrs.&quot;</span>, <span class="st">&quot;mrs&quot;</span>, text)</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>        text <span class="op">=</span> re.sub(<span class="vs">r&quot;ms.&quot;</span>, <span class="st">&quot;ms&quot;</span>, text)</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>        text <span class="op">=</span> re.sub(<span class="vs">r&quot;(\!|\?)&quot;</span>, <span class="st">&quot;.&quot;</span>, text)</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>        text <span class="op">=</span> re.sub(<span class="vs">r&quot;-&quot;</span>, <span class="st">&quot; &quot;</span>, text)</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>        text <span class="op">=</span> <span class="st">&quot;&quot;</span>.join(</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>            char <span class="cf">for</span> char <span class="kw">in</span> text <span class="cf">if</span> char <span class="kw">not</span> <span class="kw">in</span> <span class="st">&quot;</span><span class="ch">\&quot;</span><span class="st">#$%&amp;&#39;()*+,/:;&lt;=&gt;@[</span><span class="ch">\\</span><span class="st">]^_`{|}~&quot;</span></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>        text <span class="op">=</span> re.sub(<span class="vs">r&quot;\.+&quot;</span>, <span class="st">&quot;.&quot;</span>, text)</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> text</span></code></pre></div>
</section>
<section class="slide level3">

<p><code>IMDBTokenizer</code> in action</p>
<div class="sourceCode" id="cb8"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>reviews <span class="op">=</span> data[<span class="st">&quot;review&quot;</span>].tolist()</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>review <span class="op">=</span> reviews[<span class="dv">0</span>]</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> IMDBTokenizer(reviews)</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>tokenized_review <span class="op">=</span> tokenizer(review)</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>tokenised_review_decoded <span class="op">=</span> tokenizer.tokens2text(tokenized_review[:<span class="dv">10</span>])</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;ORIGINAL TEXT: </span><span class="sc">{</span>review[:<span class="dv">47</span>]<span class="sc">}</span><span class="ss"> ...&quot;</span>)</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;TOKENS FROM TEXT: </span><span class="sc">{</span><span class="st">&#39;, &#39;</span><span class="sc">.</span>join(<span class="bu">str</span>(t) <span class="cf">for</span> t <span class="kw">in</span> tokenized_review[:<span class="dv">10</span>])<span class="sc">}</span><span class="ss"> ...&quot;</span>)</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;TEXT FROM TOKENS: </span><span class="sc">{</span>tokenised_review_decoded<span class="sc">}</span><span class="ss"> ...&quot;</span>)</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a><span class="co"># ORIGINAL TEXT: Forget what I said about Emeril. Rachael Ray is ...</span></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a><span class="co"># TOKENS FROM TEXT: 831, 49, 11, 300, 44, 37877, 3, 10505, 1363, 8 ...</span></span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a><span class="co"># TEXT FROM TOKENS: forget what i said about emeril. rachael ray is ...</span></span></code></pre></div>
<p>This is adequate for the current endeavor, but serious models use
more sophisticated tokenisation algorithms, such as <a
href="https://huggingface.co/learn/nlp-course/chapter6/5?fw=pt">Byte-Pair
Encoding (BPE)</a>, which is one of the ‘secret ingredients’ of OpenAI’s
GPT models.</p>
</section>
<section id="datasets-and-dataloaders" class="slide level3">
<h3>Datasets and DataLoaders</h3>
<p>PyTorch provides a framework for composing portable data pipelines
that can be used with any model.</p>
<p><code>torch.utils.data.Dataset</code>
<code>torch.utils.data.IterableDataset</code>
<code>torch.utils.data.DataLoader</code></p>
<p>Our pipeline delivers pairs of token sequences with an offset of one
token between them.</p>
<div class="sourceCode" id="cb9"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>tokenized_reviews <span class="op">=</span> [tokenizer(review) <span class="cf">for</span> review <span class="kw">in</span> reviews]</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> FilmReviewSequences(tokenized_reviews)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>x, y <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(dataset))</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;x[:5]: </span><span class="sc">{</span>x[:<span class="dv">5</span>]<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;y[:5]: </span><span class="sc">{</span>y[:<span class="dv">5</span>]<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="co"># x[:5]: tensor([831,  49,  11, 300,  44])</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a><span class="co"># y[:5]: tensor([   49,    11,   300,    44, 37877])</span></span></code></pre></div>
</section>
<section class="slide level3">

<div class="sourceCode" id="cb10"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> FilmReviewSequences(IterableDataset):</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;IMDB film reviews for training generative models.&quot;&quot;&quot;</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>,</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>        tokenized_reviews: Iterable[<span class="bu">list</span>[<span class="bu">int</span>]],</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>        max_seq_len: <span class="bu">int</span> <span class="op">=</span> <span class="dv">40</span>,</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>        min_seq_len: <span class="bu">int</span> <span class="op">=</span> <span class="dv">20</span>,</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>        chunk_eos_token: <span class="bu">int</span> <span class="op">|</span> <span class="va">None</span> <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>        chunk_overlap: <span class="bu">bool</span> <span class="op">=</span> <span class="va">True</span>,</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>        tag: <span class="bu">str</span> <span class="op">=</span> <span class="st">&quot;data&quot;</span>,</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>    ):</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._data_file_path <span class="op">=</span> TORCH_DATA_STORAGE_PATH <span class="op">/</span> <span class="ss">f&quot;imdb_sequences_</span><span class="sc">{</span>tag<span class="sc">}</span><span class="ss">.json&quot;</span></span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> <span class="bu">open</span>(<span class="va">self</span>._data_file_path, mode<span class="op">=</span><span class="st">&quot;w&quot;</span>) <span class="im">as</span> <span class="bu">file</span>:</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> chunk_eos_token:</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> tok_review <span class="kw">in</span> tokenized_reviews:</span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>                    tok_chunks_itr <span class="op">=</span> make_chunks(</span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>                        tok_review,</span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>                        chunk_eos_token,</span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>                        max_seq_len,</span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>                        min_seq_len,</span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a>                        chunk_overlap</span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a>                    )</span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">for</span> tok_chunk <span class="kw">in</span> tok_chunks_itr:</span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a>                        <span class="bu">file</span>.write(json.dumps(tok_chunk) <span class="op">+</span> <span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>)</span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> tok_review <span class="kw">in</span> tokenized_reviews:</span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a>                    <span class="bu">file</span>.write(json.dumps(tok_review[:max_seq_len]) <span class="op">+</span> <span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>)</span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a>    ...</span></code></pre></div>
</section>
<section class="slide level3">

<div class="sourceCode" id="cb11"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> FilmReviewSequences(IterableDataset):</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;IMDB film reviews for training generative models.&quot;&quot;&quot;</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    ...</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__iter__</span>(<span class="va">self</span>) <span class="op">-&gt;</span> Iterable[<span class="bu">tuple</span>[Tensor, Tensor]]:</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> <span class="bu">open</span>(<span class="va">self</span>._data_file_path) <span class="im">as</span> <span class="bu">file</span>:</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> line <span class="kw">in</span> <span class="bu">file</span>:</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>                tokenized_chunk <span class="op">=</span> json.loads(line)</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>                <span class="cf">yield</span> (tensor(tokenized_chunk[:<span class="op">-</span><span class="dv">1</span>]), tensor(tokenized_chunk[<span class="dv">1</span>:]))</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>) <span class="op">-&gt;</span> <span class="bu">int</span>:</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> <span class="bu">open</span>(<span class="va">self</span>._data_file_path) <span class="im">as</span> <span class="bu">file</span>:</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>            num_rows <span class="op">=</span> <span class="bu">sum</span>(<span class="dv">1</span> <span class="cf">for</span> line <span class="kw">in</span> <span class="bu">file</span>)</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> num_rows</span></code></pre></div>
<p>Note, the entire dataset it not held in memory, but is loaded from
disk on-demand in an attempt to optimize memory during training.</p>
</section>
<section class="slide level3">

<p>Use <code>DataLoader</code> to batch data and handle parallelism.</p>
<div class="sourceCode" id="cb12"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> pad_seq2seq_data(batch: <span class="bu">list</span>[<span class="bu">tuple</span>[<span class="bu">int</span>, <span class="bu">int</span>]]) <span class="op">-&gt;</span> <span class="bu">tuple</span>[Tensor, Tensor]:</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Pad sequence2sequence data tuples.&quot;&quot;&quot;</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> [e[<span class="dv">0</span>] <span class="cf">for</span> e <span class="kw">in</span> batch]</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> [e[<span class="dv">1</span>] <span class="cf">for</span> e <span class="kw">in</span> batch]</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>    x_padded <span class="op">=</span> pad_sequence(x, batch_first<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>    y_padded <span class="op">=</span> pad_sequence(y, batch_first<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x_padded, y_padded</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>data_loader <span class="op">=</span> DataLoader(datasets.test_data, batch_size<span class="op">=</span><span class="dv">10</span>, collate_fn<span class="op">=</span>pad_seq2seq_data)</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>data_batches <span class="op">=</span> [batch <span class="cf">for</span> batch <span class="kw">in</span> data_loader]</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>x_batch, y_batch <span class="op">=</span> data_batches[<span class="dv">0</span>]</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;x_batch_size = </span><span class="sc">{</span>x_batch<span class="sc">.</span>size()<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;y_batch_size = </span><span class="sc">{</span>y_batch<span class="sc">.</span>size()<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a><span class="co"># x_batch_size = torch.Size([10, 38])</span></span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a><span class="co"># y_batch_size = torch.Size([10, 38])</span></span></code></pre></div>
</section>
<section id="gpus" class="slide level3">
<h3>GPUs</h3>
<p>Models were trained using one of</p>
<figure>
<img data-src="images/m1_max.png" style="width:15.0%"
alt="Apple M1 Max" />
<figcaption aria-hidden="true">Apple M1 Max</figcaption>
</figure>
<figure>
<img data-src="images/aws.png" style="width:15.0%"
alt="p3.xlarge EC2 instance with NVIDIA V100 GPU" />
<figcaption aria-hidden="true">p3.xlarge EC2 instance with NVIDIA V100
GPU</figcaption>
</figure>
</section>
<section class="slide level3">

<p>My approach was to use the best available device for a given model.
Note that sometimes <code>mps</code> is slower than <code>cpu</code>
(until Apple get their act together).</p>
<div class="sourceCode" id="cb13"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> device</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_best_device(</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>        cuda_priority: Literal[<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>] <span class="op">=</span> <span class="dv">1</span>,</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>        mps_priority: Literal[<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>] <span class="op">=</span> <span class="dv">2</span>,</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>        cpu_priority: Literal[<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>] <span class="op">=</span> <span class="dv">3</span>,</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>    ) <span class="op">-&gt;</span> device:</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Return the best device available on the machine.&quot;&quot;&quot;</span></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>    device_priorities <span class="op">=</span> <span class="bu">sorted</span>(</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>        ((<span class="st">&quot;cuda&quot;</span>, cuda_priority), (<span class="st">&quot;mps&quot;</span>, mps_priority), (<span class="st">&quot;cpu&quot;</span>, cpu_priority)),</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>        key<span class="op">=</span><span class="kw">lambda</span> e: e[<span class="dv">1</span>]</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> device_type, _ <span class="kw">in</span> device_priorities:</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> device_type <span class="op">==</span> <span class="st">&quot;cuda&quot;</span> <span class="kw">and</span> cuda.is_available():</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> device(<span class="st">&quot;cuda&quot;</span>)</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> device_type <span class="op">==</span> <span class="st">&quot;mps&quot;</span> <span class="kw">and</span> mps.is_available():</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> device(<span class="st">&quot;mps&quot;</span>)</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> device_type <span class="op">==</span> <span class="st">&quot;cpu&quot;</span>:</span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> device(<span class="st">&quot;cpu&quot;</span>)</span></code></pre></div>
</section></section>
<section>
<section id="rnn-benchmark-model" class="title-slide slide level2">
<h2>RNN benchmark model</h2>
<p><img data-src="images/ltsm.png" /></p>
</section>
<section class="slide level3">

<p>Define the model:</p>
<div class="sourceCode" id="cb14"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> Tensor, device, manual_seed, no_grad, tensor, zeros</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.nn <span class="im">import</span> LSTM, CrossEntropyLoss, Embedding, Linear, Module</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.optim <span class="im">import</span> Adam, Optimizer</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> NextWordPredictionRNN(Module):</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;LSTM for predicting the next token in a sequence.&quot;&quot;&quot;</span></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, size_vocab: <span class="bu">int</span>, size_embed: <span class="bu">int</span>, size_hidden: <span class="bu">int</span>):</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._size_hidden <span class="op">=</span> size_hidden</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._embedding <span class="op">=</span> Embedding(size_vocab, size_embed)</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._lstm <span class="op">=</span> LSTM(size_embed, size_hidden, batch_first<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._linear <span class="op">=</span> Linear(size_hidden, size_vocab)</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x: Tensor, hidden: Tensor, cell: Tensor) <span class="op">-&gt;</span> Tensor:</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>._embedding(x).unsqueeze(<span class="dv">1</span>)</span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>        out, (hidden, cell) <span class="op">=</span> <span class="va">self</span>._lstm(out, (hidden, cell))</span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>._linear(out).reshape(out.shape[<span class="dv">0</span>], <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> out, hidden, cell</span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> initialise(<span class="va">self</span>, batch_size: <span class="bu">int</span>, device_: device) <span class="op">-&gt;</span> Tuple[Tensor, Tensor]:</span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a>        hidden <span class="op">=</span> zeros(<span class="dv">1</span>, batch_size, <span class="va">self</span>._size_hidden, device<span class="op">=</span>device_)</span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a>        cell <span class="op">=</span> zeros(<span class="dv">1</span>, batch_size, <span class="va">self</span>._size_hidden, device<span class="op">=</span>device_)</span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> hidden, cell</span></code></pre></div>
</section>
<section class="slide level3">

<p>Example output:</p>
<div class="sourceCode" id="cb15"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>dummy_token <span class="op">=</span> torch.tensor([<span class="dv">42</span>])</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>hidden_t0, cell_t0 <span class="op">=</span> model.initialise(<span class="dv">1</span>, torch.device(<span class="st">&quot;cpu&quot;</span>))</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>output_token_logit, hidden_t1, cell_t1 <span class="op">=</span> model(dummy_token, hidden_t0, cell_t0)</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(output_token_logit.size())</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a><span class="co"># torch.Size([1, 69014])</span></span></code></pre></div>
<p>Note → can only process one token at a time.</p>
</section>
<section class="slide level3">

<p>Define a single training step:</p>
<div class="sourceCode" id="cb16"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> _train_step(</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>    x_batch: Tensor,</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>    y_batch: Tensor,</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>    model: Module,</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>    loss_fn: Callable[[Tensor, Tensor], Tensor],</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>    optimizer: Optimizer,</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>    device: device,</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>) <span class="op">-&gt;</span> Tensor:</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;One iteration of the training loop (for one batch).&quot;&quot;&quot;</span></span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>    model.train()</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>    batch_size, sequence_length <span class="op">=</span> x_batch.shape</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>    loss_batch <span class="op">=</span> tensor(<span class="fl">0.0</span>, device<span class="op">=</span>device)</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad(set_to_none<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a>    hidden, cell <span class="op">=</span> model.initialise(batch_size, device)</span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> n <span class="kw">in</span> <span class="bu">range</span>(sequence_length):</span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>        y_pred, hidden, cell <span class="op">=</span> model(x_batch[:, n], hidden, cell)</span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a>        loss_batch <span class="op">+=</span> loss_fn(y_pred, y_batch[:, n])</span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a>    loss_batch.backward()</span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> loss_batch <span class="op">/</span> sequence_length</span></code></pre></div>
</section>
<section class="slide level3">

<p>Define a single validation step:</p>
<div class="sourceCode" id="cb17"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="at">@no_grad</span>()</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> _val_step(</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>    x_batch: Tensor,</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>    y_batch: Tensor,</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>    model: Module,</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>    loss_fn: Callable[[Tensor, Tensor], Tensor],</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>    device: device,</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>) <span class="op">-&gt;</span> Tensor:</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;One iteration of the validation loop (for one batch).&quot;&quot;&quot;</span></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>    batch_size, sequence_length <span class="op">=</span> x_batch.shape</span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>    loss_batch <span class="op">=</span> tensor(<span class="fl">0.0</span>, device<span class="op">=</span>device)</span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a>    hidden, cell <span class="op">=</span> model.initialise(batch_size, device)</span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> n <span class="kw">in</span> <span class="bu">range</span>(sequence_length):</span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a>        y_pred, hidden, cell <span class="op">=</span> model(x_batch[:, n], hidden, cell)</span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a>        loss_batch <span class="op">+=</span> loss_fn(y_pred, y_batch[:, n])</span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> loss_batch <span class="op">/</span> sequence_length</span></code></pre></div>
</section>
<section class="slide level3">

<p>Define the full training routine:</p>
<div class="sourceCode" id="cb18"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train(</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>    model: Module,</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>    train_data: DataLoader,</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>    val_data: DataLoader,</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>    n_epochs: <span class="bu">int</span>,</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>    learning_rate: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.001</span>,</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>    random_seed: <span class="bu">int</span> <span class="op">=</span> <span class="dv">42</span>,</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>    device: device <span class="op">=</span> get_best_device(),</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>) <span class="op">-&gt;</span> Tuple[Dict[<span class="bu">int</span>, <span class="bu">float</span>], Dict[<span class="bu">int</span>, <span class="bu">float</span>], ModelCheckpoint]:</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Training loop for LTSM flavoured RNNs on sequence data.&quot;&quot;&quot;</span></span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>    manual_seed(random_seed)</span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>    model.to(device)</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a>    optimizer <span class="op">=</span> Adam(model.parameters(), lr<span class="op">=</span>learning_rate)</span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>    loss_fn <span class="op">=</span> CrossEntropyLoss(ignore_index<span class="op">=</span>PAD_TOKEN_IDX)</span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a>    train_losses: Dict[<span class="bu">int</span>, <span class="bu">float</span>] <span class="op">=</span> {}</span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a>    val_losses: Dict[<span class="bu">int</span>, <span class="bu">float</span>] <span class="op">=</span> {}</span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a>    ...</span></code></pre></div>
</section>
<section class="slide level3">

<div class="sourceCode" id="cb19"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train(...) <span class="op">-&gt;</span> Tuple[Dict[<span class="bu">int</span>, <span class="bu">float</span>], Dict[<span class="bu">int</span>, <span class="bu">float</span>], ModelCheckpoint]:</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Training loop for LTSM flavoured RNNs on sequence data.&quot;&quot;&quot;</span></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>    ...</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, n_epochs <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>        loss_train <span class="op">=</span> tensor(<span class="fl">0.0</span>).to(device)</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i, (x_batch, y_batch) <span class="kw">in</span> <span class="bu">enumerate</span>((pbar <span class="op">:=</span> tqdm(train_data)), start<span class="op">=</span><span class="dv">1</span>):</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> x_batch.to(device, non_blocking<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>            y <span class="op">=</span> y_batch.to(device, non_blocking<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>            loss_train <span class="op">+=</span> _train_step(x, y, model, loss_fn, optimizer, device)</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>            pbar.set_description(<span class="ss">f&quot;epoch </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss"> training loss = </span><span class="sc">{</span>loss_train<span class="op">/</span>i<span class="sc">:.4f}</span><span class="ss">&quot;</span>)</span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>        loss_val <span class="op">=</span> tensor(<span class="fl">0.0</span>).to(device)</span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> x_batch, y_batch <span class="kw">in</span> val_data:</span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> x_batch.to(device, non_blocking<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a>            y <span class="op">=</span> y_batch.to(device, non_blocking<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a>            loss_val <span class="op">+=</span> _val_step(x, y, model, loss_fn, device)</span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a>        epoch_train_loss <span class="op">=</span> loss_train.item() <span class="op">/</span> <span class="bu">len</span>(train_data)</span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a>        epoch_val_loss <span class="op">=</span> loss_val.item() <span class="op">/</span> <span class="bu">len</span>(val_data)</span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-23"><a href="#cb19-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> epoch <span class="op">==</span> <span class="dv">1</span> <span class="kw">or</span> epoch_val_loss <span class="op">&lt;</span> <span class="bu">min</span>(val_losses.values()):</span>
<span id="cb19-24"><a href="#cb19-24" aria-hidden="true" tabindex="-1"></a>            best_checkpoint <span class="op">=</span> ModelCheckpoint(</span>
<span id="cb19-25"><a href="#cb19-25" aria-hidden="true" tabindex="-1"></a>                epoch, epoch_train_loss, epoch_val_loss, model.state_dict().copy()</span>
<span id="cb19-26"><a href="#cb19-26" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb19-27"><a href="#cb19-27" aria-hidden="true" tabindex="-1"></a>        train_losses[epoch] <span class="op">=</span> epoch_train_loss</span>
<span id="cb19-28"><a href="#cb19-28" aria-hidden="true" tabindex="-1"></a>        val_losses[epoch] <span class="op">=</span> epoch_val_loss</span>
<span id="cb19-29"><a href="#cb19-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-30"><a href="#cb19-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> _early_stop(val_losses):</span>
<span id="cb19-31"><a href="#cb19-31" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span></span>
<span id="cb19-32"><a href="#cb19-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-33"><a href="#cb19-33" aria-hidden="true" tabindex="-1"></a>    ...</span></code></pre></div>
</section>
<section class="slide level3">

<div class="sourceCode" id="cb20"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train(...) <span class="op">-&gt;</span> Tuple[Dict[<span class="bu">int</span>, <span class="bu">float</span>], Dict[<span class="bu">int</span>, <span class="bu">float</span>], ModelCheckpoint]:</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Training loop for LTSM flavoured RNNs on sequence data.&quot;&quot;&quot;</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>    ...</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">best model:&quot;</span>)</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;|-- epoch: </span><span class="sc">{</span>best_checkpoint<span class="sc">.</span>epoch<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;|-- validation loss: </span><span class="sc">{</span>best_checkpoint<span class="sc">.</span>val_loss<span class="sc">:.4f}</span><span class="ss">&quot;</span>)</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>    model.load_state_dict(best_checkpoint.state_dict)</span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> train_losses, val_losses, best_checkpoint</span></code></pre></div>
</section>
<section class="slide level3">

<p>Train the model:</p>
<div class="sourceCode" id="cb21"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Hyper-parameters that lead to a model with 215,234 parameters.</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>SIZE_EMBED <span class="op">=</span> <span class="dv">256</span></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>SIZE_HIDDEN <span class="op">=</span> <span class="dv">512</span></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>MAX_EPOCHS <span class="op">=</span> <span class="dv">30</span></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>BATCH_SIZE <span class="op">=</span> <span class="dv">256</span></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>MAX_SEQ_LEN <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>MIN_SEQ_LEN <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>MIN_WORD_FREQ <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>LEARNING_RATE <span class="op">=</span> <span class="fl">0.005</span></span></code></pre></div>
</section>
<section class="slide level3">

<p><img data-src="images/lstm_train_stats.png"
style="width:50.0%" /></p>
<pre class="text"><code>epoch 1 training loss = 5.4329: 100%|██████████| 2105/2105 [2:31:20&lt;00:00,  4.31s/it]  
epoch 2 training loss = 4.8774: 100%|██████████| 2105/2105 [2:30:43&lt;00:00,  4.30s/it]  
epoch 3 training loss = 4.6274: 100%|██████████| 2105/2105 [2:31:16&lt;00:00,  4.31s/it]  
epoch 4 training loss = 4.4552: 100%|██████████| 2105/2105 [2:30:40&lt;00:00,  4.29s/it]  

best model:
|-- epoch: 2
|-- validation loss: 5.0893</code></pre>
</section>
<section id="text-generation-strategies" class="slide level3">
<h3>Text generation strategies</h3>
<div class="sourceCode" id="cb23"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> _sample_decoding(logits: Tensor, temperature: <span class="bu">float</span> <span class="op">=</span> <span class="fl">1.0</span>) <span class="op">-&gt;</span> Tensor:</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Generate next token using sample decoding strategy.&quot;&quot;&quot;</span></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> Categorical(logits<span class="op">=</span>logits.squeeze() <span class="op">/</span> temperature).sample()</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> _top_k_decoding(logits: Tensor, temperature: <span class="bu">float</span> <span class="op">=</span> <span class="fl">1.0</span>, k: <span class="bu">int</span> <span class="op">=</span> <span class="dv">3</span>) <span class="op">-&gt;</span> Tensor:</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Generate next token using top-k decoding strategy.&quot;&quot;&quot;</span></span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>    token_probs <span class="op">=</span> Categorical(logits<span class="op">=</span>logits.squeeze() <span class="op">/</span> temperature).probs</span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>    top_k_tokens <span class="op">=</span> topk(token_probs, k<span class="op">=</span>k)</span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>    sampled_token <span class="op">=</span> Categorical(probs<span class="op">=</span>top_k_tokens.values).sample()</span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> top_k_tokens.indices[sampled_token]</span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> _greedy_decoding(logits: Tensor, temperature: <span class="bu">float</span> <span class="op">=</span> <span class="fl">1.0</span>) <span class="op">-&gt;</span> Tensor:</span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Generate next token using greedy decoding strategy.&quot;&quot;&quot;</span></span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a>    token_probs <span class="op">=</span> Categorical(logits<span class="op">=</span>logits.squeeze() <span class="op">/</span> temperature).probs</span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> argmax(token_probs)</span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-19"><a href="#cb23-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-20"><a href="#cb23-20" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> decode(</span>
<span id="cb23-21"><a href="#cb23-21" aria-hidden="true" tabindex="-1"></a>    token_logits: Tensor,</span>
<span id="cb23-22"><a href="#cb23-22" aria-hidden="true" tabindex="-1"></a>    strategy: Literal[<span class="st">&quot;greedy&quot;</span>, <span class="st">&quot;sample&quot;</span>, <span class="st">&quot;topk&quot;</span>] <span class="op">=</span> <span class="st">&quot;greedy&quot;</span>,</span>
<span id="cb23-23"><a href="#cb23-23" aria-hidden="true" tabindex="-1"></a>    temperature: <span class="bu">float</span> <span class="op">=</span> <span class="fl">1.0</span>,</span>
<span id="cb23-24"><a href="#cb23-24" aria-hidden="true" tabindex="-1"></a>    <span class="op">*</span>,</span>
<span id="cb23-25"><a href="#cb23-25" aria-hidden="true" tabindex="-1"></a>    k: <span class="bu">int</span> <span class="op">=</span> <span class="dv">5</span>,</span>
<span id="cb23-26"><a href="#cb23-26" aria-hidden="true" tabindex="-1"></a>) <span class="op">-&gt;</span> Tensor:</span>
<span id="cb23-27"><a href="#cb23-27" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Decode generative model output using the specified strategy.&quot;&quot;&quot;</span></span>
<span id="cb23-28"><a href="#cb23-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">match</span> strategy:</span>
<span id="cb23-29"><a href="#cb23-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">case</span> <span class="st">&quot;greedy&quot;</span>:</span>
<span id="cb23-30"><a href="#cb23-30" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> _greedy_decoding(token_logits, temperature)</span>
<span id="cb23-31"><a href="#cb23-31" aria-hidden="true" tabindex="-1"></a>        <span class="cf">case</span> <span class="st">&quot;topk&quot;</span>:</span>
<span id="cb23-32"><a href="#cb23-32" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> _top_k_decoding(token_logits, temperature, k)</span>
<span id="cb23-33"><a href="#cb23-33" aria-hidden="true" tabindex="-1"></a>        <span class="cf">case</span> <span class="st">&quot;sample&quot;</span>:</span>
<span id="cb23-34"><a href="#cb23-34" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> _sample_decoding(token_logits, temperature)</span></code></pre></div>
</section>
<section id="generating-text-from-the-rnn-model" class="slide level3">
<h3>Generating text from the RNN model</h3>
<div class="sourceCode" id="cb24"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate(</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>    model: NextWordPredictionRNN,</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>    prompt: <span class="bu">str</span>,</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>    tokenizer: _Tokenizer,</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>    strategy: Literal[<span class="st">&quot;greedy&quot;</span>, <span class="st">&quot;sample&quot;</span>, <span class="st">&quot;topk&quot;</span>] <span class="op">=</span> <span class="st">&quot;greedy&quot;</span>,</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>    output_length: <span class="bu">int</span> <span class="op">=</span> <span class="dv">60</span>,</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>    temperature: <span class="bu">float</span> <span class="op">=</span> <span class="fl">1.0</span>,</span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>    random_seed: <span class="bu">int</span> <span class="op">=</span> <span class="dv">42</span>,</span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>    device: device <span class="op">=</span> get_best_device(),</span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a>    <span class="op">*</span>,</span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a>    k: <span class="bu">int</span> <span class="op">=</span> <span class="dv">2</span>,</span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a>) <span class="op">-&gt;</span> <span class="bu">str</span>:</span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Generate new text conditional on a text prompt.&quot;&quot;&quot;</span></span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a>    manual_seed(random_seed)</span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-16"><a href="#cb24-16" aria-hidden="true" tabindex="-1"></a>    model.to(device)</span>
<span id="cb24-17"><a href="#cb24-17" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb24-18"><a href="#cb24-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-19"><a href="#cb24-19" aria-hidden="true" tabindex="-1"></a>    prompt_tokens <span class="op">=</span> tokenizer(prompt)</span>
<span id="cb24-20"><a href="#cb24-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-21"><a href="#cb24-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Feed tokenised prompt into model.</span></span>
<span id="cb24-22"><a href="#cb24-22" aria-hidden="true" tabindex="-1"></a>    hidden, cell <span class="op">=</span> model.initialise(<span class="dv">1</span>, device)</span>
<span id="cb24-23"><a href="#cb24-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> token <span class="kw">in</span> prompt_tokens[:<span class="op">-</span><span class="dv">1</span>]:</span>
<span id="cb24-24"><a href="#cb24-24" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> tensor([token], device<span class="op">=</span>device)</span>
<span id="cb24-25"><a href="#cb24-25" aria-hidden="true" tabindex="-1"></a>        _, hidden, cell <span class="op">=</span> model(x, hidden, cell)</span>
<span id="cb24-26"><a href="#cb24-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-27"><a href="#cb24-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Then predict the next token, add it to the sequence, and iterate</span></span>
<span id="cb24-28"><a href="#cb24-28" aria-hidden="true" tabindex="-1"></a>    token_sequence <span class="op">=</span> prompt_tokens.copy()</span>
<span id="cb24-29"><a href="#cb24-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(output_length):</span>
<span id="cb24-30"><a href="#cb24-30" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> tensor([token_sequence[<span class="op">-</span><span class="dv">1</span>]], device<span class="op">=</span>device)</span>
<span id="cb24-31"><a href="#cb24-31" aria-hidden="true" tabindex="-1"></a>        token_logits, hidden, cell <span class="op">=</span> model(x, hidden, cell)</span>
<span id="cb24-32"><a href="#cb24-32" aria-hidden="true" tabindex="-1"></a>        token_pred <span class="op">=</span> decode(token_logits, strategy, temperature, k<span class="op">=</span>k)</span>
<span id="cb24-33"><a href="#cb24-33" aria-hidden="true" tabindex="-1"></a>        token_sequence <span class="op">+=</span> [token_pred.item()]</span>
<span id="cb24-34"><a href="#cb24-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-35"><a href="#cb24-35" aria-hidden="true" tabindex="-1"></a>    new_token_sequence <span class="op">=</span> token_sequence[<span class="bu">len</span>(prompt_tokens) :]</span>
<span id="cb24-36"><a href="#cb24-36" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> format_generated_words(tokenizer.tokens2text(new_token_sequence), prompt)</span></code></pre></div>
</section>
<section class="slide level3">

<p>Start with an untrained model as a reference point:</p>
<div class="sourceCode" id="cb25"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>prompt <span class="op">=</span> <span class="st">&quot;This is a classic horror and&quot;</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a><span class="co"># ==&gt; THIS IS A CLASSIC HORROR AND numerically unsavory aiken pyewacket nagase comparative</span></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a><span class="co"># dave compounded surfboards seemsdestined chekhov interdiction prussic hunh kosugis</span></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a><span class="co"># germanys sole filmsfor sedimentation albino 2036 krug zefferelli djalili baldwins chowder</span></span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a><span class="co"># strauss shutes haifa seeming 101st mrbumble grandmas noll bulgarias lenders repressed</span></span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a><span class="co"># deneuve ounce emphasise salome tracking avian mrmyagi megalopolis countries dolorous</span></span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a><span class="co"># fairview dying subtitle appointed dollar opting energized tremell cya slinging riot</span></span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a><span class="co"># seemsslow secaucus muco forgo mediation patio flogs armsin sbaraglia snowflake usurps</span></span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a><span class="co"># roadmovie slogans holy vanishes zuckers herrmann encyclopedia dorma chapas fairview whit</span></span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a><span class="co"># mergers katie motherhood ejaculation stepehn nat unremitting munched munched sceneand</span></span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a><span class="co"># jarhead skaal broadcasted pottery admonition lewbert upholding neat projectile bjork...</span></span></code></pre></div>
</section>
<section class="slide level3">

<p>Then take a look at what a top-5 decoding strategy yields with the
trained model:</p>
<div class="sourceCode" id="cb26"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>prompt <span class="op">=</span> <span class="st">&quot;This is a classic horror and&quot;</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a><span class="co"># ==&gt; THIS IS A CLASSIC HORROR AND the story is a bit of a letdown. The story is told in</span></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a><span class="co"># some ways. The only redeeming feature in the whole movie is that its not a good idea. Its</span></span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a><span class="co"># a wonderful story with a very limited performance and the music and the script. The story</span></span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a><span class="co"># is not that bad. The story is not a spoiler. The story is a little slow but its not the</span></span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a><span class="co"># best one to come to mind of mencia. Its not a movie to watch. It is a good film to watch.</span></span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Its not....</span></span></code></pre></div>
</section></section>
<section>
<section id="generative-decoder-model" class="title-slide slide level2">
<h2>Generative decoder model</h2>
<p><img data-src="images/encoder_decoder_mod.png"
style="width:45.0%" /></p>
</section>
<section id="positional-encoding" class="slide level3">
<h3>Positional encoding</h3>
<div class="sourceCode" id="cb27"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> PositionalEncoding(Module):</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Position encoder taken from &#39;Attention is all you Need&#39;.&quot;&quot;&quot;</span></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, size_embed: <span class="bu">int</span>, dropout: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.1</span>, max_seq_len: <span class="bu">int</span> <span class="op">=</span> <span class="dv">1000</span>):</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._dropout <span class="op">=</span> Dropout(p<span class="op">=</span>dropout)</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>        position <span class="op">=</span> arange(max_seq_len).unsqueeze(<span class="dv">1</span>)</span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a>        div_term <span class="op">=</span> exp(arange(<span class="dv">0</span>, size_embed, <span class="dv">2</span>) <span class="op">*</span> (<span class="op">-</span>log(tensor(<span class="fl">10000.0</span>)) <span class="op">/</span> size_embed))</span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a>        pos_encoding <span class="op">=</span> zeros(max_seq_len, size_embed)</span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a>        pos_encoding[:, <span class="dv">0</span>::<span class="dv">2</span>] <span class="op">=</span> sin(position <span class="op">*</span> div_term)</span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a>        pos_encoding[:, <span class="dv">1</span>::<span class="dv">2</span>] <span class="op">=</span> cos(position <span class="op">*</span> div_term)</span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.register_buffer(<span class="st">&quot;_pos_encoding&quot;</span>, pos_encoding)  <span class="co"># don&#39;t train these</span></span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x: Tensor) <span class="op">-&gt;</span> Tensor:</span>
<span id="cb27-16"><a href="#cb27-16" aria-hidden="true" tabindex="-1"></a>        <span class="co">&quot;&quot;&quot;</span></span>
<span id="cb27-17"><a href="#cb27-17" aria-hidden="true" tabindex="-1"></a><span class="co">        Arguments:</span></span>
<span id="cb27-18"><a href="#cb27-18" aria-hidden="true" tabindex="-1"></a><span class="co">            x: Tensor, shape ``[batch_size, seq_len, embedding_dim]``</span></span>
<span id="cb27-19"><a href="#cb27-19" aria-hidden="true" tabindex="-1"></a><span class="co">        &quot;&quot;&quot;</span></span>
<span id="cb27-20"><a href="#cb27-20" aria-hidden="true" tabindex="-1"></a>        seq_len <span class="op">=</span> x.size(<span class="dv">1</span>)</span>
<span id="cb27-21"><a href="#cb27-21" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>._pos_encoding[:seq_len]</span>
<span id="cb27-22"><a href="#cb27-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>._dropout(x)</span></code></pre></div>
<p>Analagous to adding a watermark to each embedded token, to indicate
its position in the sequence.</p>
</section>
<section class="slide level3">

<p>Define the model:</p>
<div class="sourceCode" id="cb28"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> NextWordPredictionTransformer(Module):</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Transformer for predicting the next tokens in a sequence.&quot;&quot;&quot;</span></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, size_vocab: <span class="bu">int</span>, size_embed: <span class="bu">int</span>, n_heads: <span class="bu">int</span> <span class="op">=</span> <span class="dv">1</span>):</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._size_vocab <span class="op">=</span> size_vocab</span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._size_embed <span class="op">=</span> size_embed</span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._n_heads <span class="op">=</span> n_heads</span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._position_encoder <span class="op">=</span> PositionalEncoding(size_embed)</span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._embedding <span class="op">=</span> Embedding(size_vocab, size_embed)</span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._decoder <span class="op">=</span> TransformerDecoderLayer(</span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a>            size_embed, n_heads, dim_feedforward<span class="op">=</span><span class="dv">2</span> <span class="op">*</span> size_embed, batch_first<span class="op">=</span><span class="va">True</span></span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb28-14"><a href="#cb28-14" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._linear <span class="op">=</span> Linear(size_embed, size_vocab)</span>
<span id="cb28-15"><a href="#cb28-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._init_weights()</span>
<span id="cb28-16"><a href="#cb28-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-17"><a href="#cb28-17" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x: Tensor) <span class="op">-&gt;</span> Tensor:</span>
<span id="cb28-18"><a href="#cb28-18" aria-hidden="true" tabindex="-1"></a>        x_causal_mask, x_padding_mask <span class="op">=</span> <span class="va">self</span>._make_mask(x)</span>
<span id="cb28-19"><a href="#cb28-19" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>._embedding(x) <span class="op">*</span> sqrt(tensor(<span class="va">self</span>._size_embed))</span>
<span id="cb28-20"><a href="#cb28-20" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>._position_encoder(out)</span>
<span id="cb28-21"><a href="#cb28-21" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>._decoder(</span>
<span id="cb28-22"><a href="#cb28-22" aria-hidden="true" tabindex="-1"></a>            out,</span>
<span id="cb28-23"><a href="#cb28-23" aria-hidden="true" tabindex="-1"></a>            out,</span>
<span id="cb28-24"><a href="#cb28-24" aria-hidden="true" tabindex="-1"></a>            tgt_mask<span class="op">=</span>x_causal_mask,</span>
<span id="cb28-25"><a href="#cb28-25" aria-hidden="true" tabindex="-1"></a>            tgt_key_padding_mask<span class="op">=</span>x_padding_mask,</span>
<span id="cb28-26"><a href="#cb28-26" aria-hidden="true" tabindex="-1"></a>            memory_mask<span class="op">=</span>x_causal_mask,</span>
<span id="cb28-27"><a href="#cb28-27" aria-hidden="true" tabindex="-1"></a>            memory_key_padding_mask<span class="op">=</span>x_padding_mask,</span>
<span id="cb28-28"><a href="#cb28-28" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb28-29"><a href="#cb28-29" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>._linear(out)</span>
<span id="cb28-30"><a href="#cb28-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> out</span>
<span id="cb28-31"><a href="#cb28-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-32"><a href="#cb28-32" aria-hidden="true" tabindex="-1"></a>    ...</span></code></pre></div>
</section>
<section class="slide level3">

<div class="sourceCode" id="cb29"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> NextWordPredictionTransformer(Module):</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Transformer for predicting the next tokens in a sequence.&quot;&quot;&quot;</span></span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>    ...</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _init_weights(<span class="va">self</span>) <span class="op">-&gt;</span> NextWordPredictionTransformer:</span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a>        <span class="co">&quot;&quot;&quot;Parameter initialisaion from Attention is all you Need.&quot;&quot;&quot;</span></span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> p <span class="kw">in</span> <span class="va">self</span>.parameters():</span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> p.dim() <span class="op">&gt;</span> <span class="dv">1</span>:</span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a>                xavier_uniform_(p)</span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span></span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _make_mask(<span class="va">self</span>, x: Tensor) <span class="op">-&gt;</span> <span class="bu">tuple</span>[Tensor, Tensor]:</span>
<span id="cb29-14"><a href="#cb29-14" aria-hidden="true" tabindex="-1"></a>        <span class="co">&quot;&quot;&quot;Make causal and padding masks.&quot;&quot;&quot;</span></span>
<span id="cb29-15"><a href="#cb29-15" aria-hidden="true" tabindex="-1"></a>        causal_mask <span class="op">=</span> ones(x.size(<span class="dv">0</span>) <span class="op">*</span> <span class="va">self</span>._n_heads, x.size(<span class="dv">1</span>), x.size(<span class="dv">1</span>))</span>
<span id="cb29-16"><a href="#cb29-16" aria-hidden="true" tabindex="-1"></a>        causal_mask <span class="op">=</span> tril(causal_mask) <span class="op">==</span> <span class="dv">0</span></span>
<span id="cb29-17"><a href="#cb29-17" aria-hidden="true" tabindex="-1"></a>        padding_mask <span class="op">=</span> x <span class="op">==</span> PAD_TOKEN_IDX</span>
<span id="cb29-18"><a href="#cb29-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> causal_mask.to(x.device), padding_mask.to(x.device)</span></code></pre></div>
</section>
<section class="slide level3">

<p>Example output:</p>
<div class="sourceCode" id="cb30"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>dummy_token_sequence <span class="op">=</span> torch.tensor([[<span class="dv">42</span>, <span class="dv">42</span>, <span class="dv">42</span>]])</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>output_seq_logits <span class="op">=</span> model(dummy_token_sequence)</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(output_seq_logits.size())</span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a><span class="co"># torch.Size([1, 3, 69014])</span></span></code></pre></div>
<p>Note → can process entire sequences at once.</p>
</section>
<section class="slide level3">

<p>Define a single training step:</p>
<div class="sourceCode" id="cb31"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> _train_step(</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>    x_batch: Tensor,</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>    y_batch: Tensor,</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>    model: Module,</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>    loss_fn: Callable[[Tensor, Tensor], Tensor],</span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a>    optimizer: Optimizer,</span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a>    lr_scheduler: LRScheduler,</span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a>    clip_grads: <span class="bu">float</span> <span class="op">|</span> <span class="va">None</span> <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb31-9"><a href="#cb31-9" aria-hidden="true" tabindex="-1"></a>) <span class="op">-&gt;</span> Tensor:</span>
<span id="cb31-10"><a href="#cb31-10" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;One iteration of the training loop (for one batch).&quot;&quot;&quot;</span></span>
<span id="cb31-11"><a href="#cb31-11" aria-hidden="true" tabindex="-1"></a>    model.train()</span>
<span id="cb31-12"><a href="#cb31-12" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> model(x_batch)</span>
<span id="cb31-13"><a href="#cb31-13" aria-hidden="true" tabindex="-1"></a>    loss_batch <span class="op">=</span> loss_fn(y_pred.permute(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">1</span>), y_batch)</span>
<span id="cb31-14"><a href="#cb31-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-15"><a href="#cb31-15" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad(set_to_none<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb31-16"><a href="#cb31-16" aria-hidden="true" tabindex="-1"></a>    loss_batch.backward()</span>
<span id="cb31-17"><a href="#cb31-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> clip_grads:</span>
<span id="cb31-18"><a href="#cb31-18" aria-hidden="true" tabindex="-1"></a>        clip_grad_norm_(model.parameters(), clip_grads)</span>
<span id="cb31-19"><a href="#cb31-19" aria-hidden="true" tabindex="-1"></a>    optimizer.step()</span>
<span id="cb31-20"><a href="#cb31-20" aria-hidden="true" tabindex="-1"></a>    lr_scheduler.step()</span>
<span id="cb31-21"><a href="#cb31-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-22"><a href="#cb31-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> loss_batch</span></code></pre></div>
</section>
<section class="slide level3">

<p>Define a single validation step:</p>
<div class="sourceCode" id="cb32"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="at">@no_grad</span>()</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> _val_step(</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>    x_batch: Tensor,</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>    y_batch: Tensor,</span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>    model: Module,</span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>    loss_fn: Callable[[Tensor, Tensor], Tensor],</span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a>) <span class="op">-&gt;</span> Tensor:</span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;One iteration of the validation loop (for one batch).&quot;&quot;&quot;</span></span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> model(x_batch)</span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a>    loss_batch <span class="op">=</span> loss_fn(y_pred.permute(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">1</span>), y_batch)</span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> loss_batch</span></code></pre></div>
</section>
<section class="slide level3">

<p>Set a learning rate schedule:</p>
<p><img data-src="images/lr_schedule.png" /></p>
<div class="sourceCode" id="cb33"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> warmup_schedule(step: <span class="bu">int</span>, warmup_steps: <span class="bu">int</span>, max_steps: <span class="bu">int</span>):</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Learning rate schedule function taken from GPT-1 paper.&quot;&quot;&quot;</span></span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>    lr_factor <span class="op">=</span> <span class="fl">0.5</span> <span class="op">*</span> (<span class="dv">1</span> <span class="op">+</span> math.cos(math.pi <span class="op">*</span> step <span class="op">/</span> max_steps))</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> step <span class="op">&lt;=</span> warmup_steps:</span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a>        lr_factor <span class="op">*=</span> step <span class="op">/</span> warmup_steps</span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> lr_factor</span></code></pre></div>
</section>
<section class="slide level3">

<p>Define the full training routine:</p>
<div class="sourceCode" id="cb34"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train(</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>    model: Module,</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>    train_data: DataLoader,</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>    val_data: DataLoader,</span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>    n_epochs: <span class="bu">int</span>,</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a>    learning_rate: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.001</span>,</span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a>    warmup_epochs: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.5</span>,</span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>    clip_grads: <span class="bu">float</span> <span class="op">|</span> <span class="va">None</span> <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a>    random_seed: <span class="bu">int</span> <span class="op">=</span> <span class="dv">42</span>,</span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a>    device: device <span class="op">=</span> get_best_device(cuda_priority<span class="op">=</span><span class="dv">1</span>, mps_priority<span class="op">=</span><span class="dv">3</span>, cpu_priority<span class="op">=</span><span class="dv">2</span>),</span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a>) <span class="op">-&gt;</span> Tuple[<span class="bu">dict</span>[<span class="bu">int</span>, <span class="bu">float</span>], <span class="bu">dict</span>[<span class="bu">int</span>, <span class="bu">float</span>], ModelCheckpoint]:</span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Training loop for transformer decoder.&quot;&quot;&quot;</span></span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a>    manual_seed(random_seed)</span>
<span id="cb34-14"><a href="#cb34-14" aria-hidden="true" tabindex="-1"></a>    model.to(device)</span>
<span id="cb34-15"><a href="#cb34-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-16"><a href="#cb34-16" aria-hidden="true" tabindex="-1"></a>    optimizer <span class="op">=</span> Adam(model.parameters(), lr<span class="op">=</span>learning_rate)</span>
<span id="cb34-17"><a href="#cb34-17" aria-hidden="true" tabindex="-1"></a>    loss_fn <span class="op">=</span> CrossEntropyLoss(ignore_index<span class="op">=</span>PAD_TOKEN_IDX)</span>
<span id="cb34-18"><a href="#cb34-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-19"><a href="#cb34-19" aria-hidden="true" tabindex="-1"></a>    n_batches <span class="op">=</span> <span class="bu">len</span>(train_data)</span>
<span id="cb34-20"><a href="#cb34-20" aria-hidden="true" tabindex="-1"></a>    n_warmup_steps <span class="op">=</span> math.floor(warmup_epochs <span class="op">*</span> n_batches)</span>
<span id="cb34-21"><a href="#cb34-21" aria-hidden="true" tabindex="-1"></a>    n_steps <span class="op">=</span> n_epochs <span class="op">*</span> n_batches</span>
<span id="cb34-22"><a href="#cb34-22" aria-hidden="true" tabindex="-1"></a>    lrs_fn <span class="op">=</span> partial(warmup_schedule, warmup_steps<span class="op">=</span>n_warmup_steps, max_steps<span class="op">=</span>n_steps)</span>
<span id="cb34-23"><a href="#cb34-23" aria-hidden="true" tabindex="-1"></a>    lrs <span class="op">=</span> LambdaLR(optimizer, lrs_fn)</span>
<span id="cb34-24"><a href="#cb34-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-25"><a href="#cb34-25" aria-hidden="true" tabindex="-1"></a>    train_losses: <span class="bu">dict</span>[<span class="bu">int</span>, <span class="bu">float</span>] <span class="op">=</span> {}</span>
<span id="cb34-26"><a href="#cb34-26" aria-hidden="true" tabindex="-1"></a>    val_losses: <span class="bu">dict</span>[<span class="bu">int</span>, <span class="bu">float</span>] <span class="op">=</span> {}</span>
<span id="cb34-27"><a href="#cb34-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-28"><a href="#cb34-28" aria-hidden="true" tabindex="-1"></a>    ...</span></code></pre></div>
</section>
<section class="slide level3">

<div class="sourceCode" id="cb35"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train(...) <span class="op">-&gt;</span> Tuple[Dict[<span class="bu">int</span>, <span class="bu">float</span>], Dict[<span class="bu">int</span>, <span class="bu">float</span>], ModelCheckpoint]:</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Training loop for LTSM flavoured RNNs on sequence data.&quot;&quot;&quot;</span></span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>    ...</span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;number of warmup steps: </span><span class="sc">{</span>n_warmup_steps<span class="sc">}</span><span class="ss"> / </span><span class="sc">{</span>n_steps<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, n_epochs <span class="op">+</span> <span class="dv">1</span>):</span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a>        loss_train <span class="op">=</span> tensor(<span class="fl">0.0</span>).to(device)</span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i, (x_batch, y_batch) <span class="kw">in</span> <span class="bu">enumerate</span>((pbar <span class="op">:=</span> tqdm(train_data)), start<span class="op">=</span><span class="dv">1</span>):</span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> x_batch.to(device, non_blocking<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb35-11"><a href="#cb35-11" aria-hidden="true" tabindex="-1"></a>            y <span class="op">=</span> y_batch.to(device, non_blocking<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb35-12"><a href="#cb35-12" aria-hidden="true" tabindex="-1"></a>            loss_train <span class="op">+=</span> _train_step(x, y, model, loss_fn, optimizer, lrs, clip_grads)</span>
<span id="cb35-13"><a href="#cb35-13" aria-hidden="true" tabindex="-1"></a>            lr <span class="op">=</span> lrs.get_last_lr()[<span class="dv">0</span>]</span>
<span id="cb35-14"><a href="#cb35-14" aria-hidden="true" tabindex="-1"></a>            pbar.set_description(</span>
<span id="cb35-15"><a href="#cb35-15" aria-hidden="true" tabindex="-1"></a>                <span class="ss">f&quot;epoch </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss"> training loss = </span><span class="sc">{</span>loss_train<span class="op">/</span>i<span class="sc">:.4f}</span><span class="ss"> (LR = </span><span class="sc">{</span>lr<span class="sc">:.8f}</span><span class="ss">)&quot;</span></span>
<span id="cb35-16"><a href="#cb35-16" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb35-17"><a href="#cb35-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-18"><a href="#cb35-18" aria-hidden="true" tabindex="-1"></a>        loss_val <span class="op">=</span> tensor(<span class="fl">0.0</span>).to(device)</span>
<span id="cb35-19"><a href="#cb35-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> x_batch, y_batch <span class="kw">in</span> val_data:</span>
<span id="cb35-20"><a href="#cb35-20" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> x_batch.to(device, non_blocking<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb35-21"><a href="#cb35-21" aria-hidden="true" tabindex="-1"></a>            y <span class="op">=</span> y_batch.to(device, non_blocking<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb35-22"><a href="#cb35-22" aria-hidden="true" tabindex="-1"></a>            loss_val <span class="op">+=</span> _val_step(x, y, model, loss_fn)</span>
<span id="cb35-23"><a href="#cb35-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-24"><a href="#cb35-24" aria-hidden="true" tabindex="-1"></a>        epoch_train_loss <span class="op">=</span> loss_train.item() <span class="op">/</span> <span class="bu">len</span>(train_data)</span>
<span id="cb35-25"><a href="#cb35-25" aria-hidden="true" tabindex="-1"></a>        epoch_val_loss <span class="op">=</span> loss_val.item() <span class="op">/</span> <span class="bu">len</span>(val_data)</span>
<span id="cb35-26"><a href="#cb35-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-27"><a href="#cb35-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> epoch <span class="op">==</span> <span class="dv">1</span> <span class="kw">or</span> epoch_val_loss <span class="op">&lt;</span> <span class="bu">min</span>(val_losses.values()):</span>
<span id="cb35-28"><a href="#cb35-28" aria-hidden="true" tabindex="-1"></a>            best_checkpoint <span class="op">=</span> ModelCheckpoint(</span>
<span id="cb35-29"><a href="#cb35-29" aria-hidden="true" tabindex="-1"></a>                epoch, epoch_train_loss, epoch_val_loss, model.state_dict().copy()</span>
<span id="cb35-30"><a href="#cb35-30" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb35-31"><a href="#cb35-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-32"><a href="#cb35-32" aria-hidden="true" tabindex="-1"></a>        train_losses[epoch] <span class="op">=</span> epoch_train_loss</span>
<span id="cb35-33"><a href="#cb35-33" aria-hidden="true" tabindex="-1"></a>        val_losses[epoch] <span class="op">=</span> epoch_val_loss</span>
<span id="cb35-34"><a href="#cb35-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-35"><a href="#cb35-35" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> _early_stop(val_losses):</span>
<span id="cb35-36"><a href="#cb35-36" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span></span>
<span id="cb35-37"><a href="#cb35-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-38"><a href="#cb35-38" aria-hidden="true" tabindex="-1"></a>    ...</span></code></pre></div>
</section>
<section class="slide level3">

<div class="sourceCode" id="cb36"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> train(...) <span class="op">-&gt;</span> Tuple[Dict[<span class="bu">int</span>, <span class="bu">float</span>], Dict[<span class="bu">int</span>, <span class="bu">float</span>], ModelCheckpoint]:</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Training loop for LTSM flavoured RNNs on sequence data.&quot;&quot;&quot;</span></span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a>    ...</span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">best model:&quot;</span>)</span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;|-- epoch: </span><span class="sc">{</span>best_checkpoint<span class="sc">.</span>epoch<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;|-- validation loss: </span><span class="sc">{</span>best_checkpoint<span class="sc">.</span>val_loss<span class="sc">:.4f}</span><span class="ss">&quot;</span>)</span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a>    model.load_state_dict(best_checkpoint.state_dict)</span>
<span id="cb36-11"><a href="#cb36-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> train_losses, val_losses, best_checkpoint</span></code></pre></div>
</section>
<section class="slide level3">

<p>Train the model:</p>
<div class="sourceCode" id="cb37"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Hyper-parameters that lead to a model with 214,210 parameters.</span></span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>SIZE_EMBED <span class="op">=</span> <span class="dv">256</span></span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a>MAX_EPOCHS <span class="op">=</span> <span class="dv">30</span></span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a>BATCH_SIZE <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a>MAX_SEQ_LEN <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a>MIN_SEQ_LEN <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a>MIN_WORD_FREQ <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true" tabindex="-1"></a>MAX_LEARNING_RATE <span class="op">=</span> <span class="fl">0.001</span></span>
<span id="cb37-10"><a href="#cb37-10" aria-hidden="true" tabindex="-1"></a>WARMUP_EPOCHS <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb37-11"><a href="#cb37-11" aria-hidden="true" tabindex="-1"></a>GRADIENT_CLIP <span class="op">=</span> <span class="dv">5</span></span></code></pre></div>
</section>
<section class="slide level3">

<p><img data-src="images/decoder_train_stats.png"
style="width:50.0%" /></p>
<pre class="text"><code>number of warmup steps: 33692 / 505380
epoch 1 training loss = 5.9463 (LR = 0.00049863): 100%|██████████| 16846/16846 [51:35&lt;00:00,  5.44it/s]
epoch 2 training loss = 5.1662 (LR = 0.00098907): 100%|██████████| 16846/16846 [51:32&lt;00:00,  5.45it/s]
epoch 3 training loss = 5.0460 (LR = 0.00097553): 100%|██████████| 16846/16846 [51:32&lt;00:00,  5.45it/s]
epoch 4 training loss = 4.9569 (LR = 0.00095677): 100%|██████████| 16846/16846 [51:32&lt;00:00,  5.45it/s]
epoch 5 training loss = 4.9277 (LR = 0.00093301): 100%|██████████| 16846/16846 [51:33&lt;00:00,  5.45it/s]

best model:
|-- epoch: 3
|-- validation loss: 5.0740</code></pre>
</section>
<section id="generating-text-from-the-decoder-model"
class="slide level3">
<h3>Generating text from the decoder model</h3>
<div class="sourceCode" id="cb39"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate(</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>    model: NextWordPredictionTransformer,</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a>    prompt: <span class="bu">str</span>,</span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a>    tokenizer: _Tokenizer,</span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a>    strategy: Literal[<span class="st">&quot;greedy&quot;</span>, <span class="st">&quot;sample&quot;</span>, <span class="st">&quot;topk&quot;</span>] <span class="op">=</span> <span class="st">&quot;greedy&quot;</span>,</span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a>    output_length: <span class="bu">int</span> <span class="op">=</span> <span class="dv">60</span>,</span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a>    temperature: <span class="bu">float</span> <span class="op">=</span> <span class="fl">1.0</span>,</span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a>    random_seed: <span class="bu">int</span> <span class="op">=</span> <span class="dv">42</span>,</span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a>    device: device <span class="op">=</span> get_best_device(),</span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a>    <span class="op">*</span>,</span>
<span id="cb39-11"><a href="#cb39-11" aria-hidden="true" tabindex="-1"></a>    k: <span class="bu">int</span> <span class="op">=</span> <span class="dv">2</span>,</span>
<span id="cb39-12"><a href="#cb39-12" aria-hidden="true" tabindex="-1"></a>) <span class="op">-&gt;</span> <span class="bu">str</span>:</span>
<span id="cb39-13"><a href="#cb39-13" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Generate new text conditional on a text prompt.&quot;&quot;&quot;</span></span>
<span id="cb39-14"><a href="#cb39-14" aria-hidden="true" tabindex="-1"></a>    manual_seed(random_seed)</span>
<span id="cb39-15"><a href="#cb39-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-16"><a href="#cb39-16" aria-hidden="true" tabindex="-1"></a>    model.to(device)</span>
<span id="cb39-17"><a href="#cb39-17" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb39-18"><a href="#cb39-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-19"><a href="#cb39-19" aria-hidden="true" tabindex="-1"></a>    prompt_tokens <span class="op">=</span> tokenizer(prompt)</span>
<span id="cb39-20"><a href="#cb39-20" aria-hidden="true" tabindex="-1"></a>    token_sequence <span class="op">=</span> prompt_tokens.copy()</span>
<span id="cb39-21"><a href="#cb39-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(output_length):</span>
<span id="cb39-22"><a href="#cb39-22" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> tensor([token_sequence], device<span class="op">=</span>device)</span>
<span id="cb39-23"><a href="#cb39-23" aria-hidden="true" tabindex="-1"></a>        token_logits <span class="op">=</span> model(x)</span>
<span id="cb39-24"><a href="#cb39-24" aria-hidden="true" tabindex="-1"></a>        token_pred <span class="op">=</span> decode(token_logits[<span class="dv">0</span>, <span class="op">-</span><span class="dv">1</span>], strategy, temperature, k<span class="op">=</span>k)</span>
<span id="cb39-25"><a href="#cb39-25" aria-hidden="true" tabindex="-1"></a>        token_sequence <span class="op">+=</span> [token_pred.item()]</span>
<span id="cb39-26"><a href="#cb39-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-27"><a href="#cb39-27" aria-hidden="true" tabindex="-1"></a>    new_token_sequence <span class="op">=</span> token_sequence[<span class="bu">len</span>(prompt_tokens) :]</span>
<span id="cb39-28"><a href="#cb39-28" aria-hidden="true" tabindex="-1"></a>    new_token_sequence <span class="op">=</span> token_sequence[<span class="bu">len</span>(prompt_tokens) :]</span>
<span id="cb39-29"><a href="#cb39-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> format_generated_words(tokenizer.tokens2text(new_token_sequence), prompt)</span></code></pre></div>
</section>
<section class="slide level3">

<p>Start with an untrained model as a reference point:</p>
<div class="sourceCode" id="cb40"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>prompt <span class="op">=</span> <span class="st">&quot;This is a classic horror and&quot;</span></span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a><span class="co"># ==&gt; THIS IS A CLASSIC HORROR AND tsa tsa tsa wiimote wiimote upclose upclose upclose</span></span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a><span class="co"># naturalism upfront upfront upfront 1930the punctuation indiscernible upfront upfront</span></span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a><span class="co"># upfront upfront upfront upfront upfront granting whining nazarin certo certo certo</span></span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a><span class="co"># upfront perine perine centralized neurological neurological neurological crestfallen</span></span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a><span class="co"># crestfallen allfor neurological neurological neurological cassavetess perine perine</span></span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a><span class="co"># laughter laughter laughter laughter certo certo yorkavant yorkavant lacing lacing lacing</span></span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a><span class="co"># lacing allfor boredome yorkavant boredome yorkavant jobmore savannahs neurological</span></span>
<span id="cb40-10"><a href="#cb40-10" aria-hidden="true" tabindex="-1"></a><span class="co"># neurological lunchmeat badmen yorkavant yorkavant yorkavant thousands thousands thousands</span></span>
<span id="cb40-11"><a href="#cb40-11" aria-hidden="true" tabindex="-1"></a><span class="co"># thousands thousands yorkavant thousands thousands forego forego world 1930the 1930the</span></span>
<span id="cb40-12"><a href="#cb40-12" aria-hidden="true" tabindex="-1"></a><span class="co"># 1930the 1930the 1930the 1930the world world thousands kinkle centralized centralized</span></span>
<span id="cb40-13"><a href="#cb40-13" aria-hidden="true" tabindex="-1"></a><span class="co"># centralized earnings earnings earnings allyway allyway xian...</span></span></code></pre></div>
</section>
<section class="slide level3">

<p>Then take a look at what a top-5 decoding strategy yields with the
trained model:</p>
<div class="sourceCode" id="cb41"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>prompt <span class="op">=</span> <span class="st">&quot;This is a classic horror and&quot;</span></span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a><span class="co"># ==&gt; THIS IS A CLASSIC HORROR AND a good story with the great cast. Its a shame that the</span></span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a><span class="co"># story has been a bit of one night of my favourite actors. This is an amazing and it was</span></span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a><span class="co"># very good for the film to watch but it has a few decent moments that is not even the best</span></span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a><span class="co"># part in the entire film but this one was a good movie for a little long after all of it</span></span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a><span class="co"># it is so much more about this. If you havent already see a lot to see it. I dont. It is.</span></span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a><span class="co"># I recommend...</span></span></code></pre></div>
</section></section>
<section>
<section id="exciting-things-to-try-with-this-llm"
class="title-slide slide level2">
<h2>Exciting things to try with this LLM</h2>

</section>
<section id="semantic-search" class="slide level3">
<h3>Semantic Search</h3>
<div class="sourceCode" id="cb42"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> DocumentEmbeddingTransformer(tfr.NextWordPredictionTransformer):</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Adapting a generative model to yield text embeddings.&quot;&quot;&quot;</span></span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, pre_trained_model: tfr.NextWordPredictionTransformer):</span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(</span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a>            pre_trained_model._size_vocab,</span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a>            pre_trained_model._size_embed,</span>
<span id="cb42-8"><a href="#cb42-8" aria-hidden="true" tabindex="-1"></a>            pre_trained_model._n_heads,</span>
<span id="cb42-9"><a href="#cb42-9" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb42-10"><a href="#cb42-10" aria-hidden="true" tabindex="-1"></a>        <span class="kw">del</span> <span class="va">self</span>._linear</span>
<span id="cb42-11"><a href="#cb42-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.load_state_dict(pre_trained_model.state_dict(), strict<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb42-12"><a href="#cb42-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-13"><a href="#cb42-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x: torch.Tensor) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb42-14"><a href="#cb42-14" aria-hidden="true" tabindex="-1"></a>        x_causal_mask, x_padding_mask <span class="op">=</span> <span class="va">self</span>._make_mask(x)</span>
<span id="cb42-15"><a href="#cb42-15" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>._embedding(x) <span class="op">*</span> math.sqrt(torch.tensor(<span class="va">self</span>._size_embed))</span>
<span id="cb42-16"><a href="#cb42-16" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>._position_encoder(out)</span>
<span id="cb42-17"><a href="#cb42-17" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>._decoder(</span>
<span id="cb42-18"><a href="#cb42-18" aria-hidden="true" tabindex="-1"></a>            out,</span>
<span id="cb42-19"><a href="#cb42-19" aria-hidden="true" tabindex="-1"></a>            out,</span>
<span id="cb42-20"><a href="#cb42-20" aria-hidden="true" tabindex="-1"></a>            tgt_mask<span class="op">=</span>x_causal_mask,</span>
<span id="cb42-21"><a href="#cb42-21" aria-hidden="true" tabindex="-1"></a>            tgt_key_padding_mask<span class="op">=</span>x_padding_mask,</span>
<span id="cb42-22"><a href="#cb42-22" aria-hidden="true" tabindex="-1"></a>            memory_mask<span class="op">=</span>x_causal_mask,</span>
<span id="cb42-23"><a href="#cb42-23" aria-hidden="true" tabindex="-1"></a>            memory_key_padding_mask<span class="op">=</span>x_padding_mask,</span>
<span id="cb42-24"><a href="#cb42-24" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb42-25"><a href="#cb42-25" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> torch.<span class="bu">sum</span>(out.squeeze(), dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb42-26"><a href="#cb42-26" aria-hidden="true" tabindex="-1"></a>        out <span class="op">/=</span> out.norm()</span>
<span id="cb42-27"><a href="#cb42-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> out</span></code></pre></div>
</section>
<section class="slide level3">

<p>Use adapted pre-trained model to index documents:</p>
<div class="sourceCode" id="cb43"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>embeddings_db <span class="op">=</span> []</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>embedding_model.<span class="bu">eval</span>()</span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, review <span class="kw">in</span> <span class="bu">enumerate</span>(reviews):</span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a>        review_tokenized <span class="op">=</span> tokenizer(reviews[i])[:CHUNK_SIZE]</span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a>        review_embedding <span class="op">=</span> embedding_model(torch.tensor([review_tokenized]))</span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a>        embeddings_db.append(review_embedding)</span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true" tabindex="-1"></a>embeddings_db <span class="op">=</span> torch.stack(embeddings_db)</span></code></pre></div>
</section>
<section class="slide level3">

<p>Use cosine similarity to process queries:</p>
<div class="sourceCode" id="cb44"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>query <span class="op">=</span> <span class="st">&quot;Classic horror movie that is terrifying&quot;</span></span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Embed the query using the model.</span></span>
<span id="cb44-4"><a href="#cb44-4" aria-hidden="true" tabindex="-1"></a>query_embedding <span class="op">=</span> embedding_model(torch.tensor([tokenizer(query)]))</span>
<span id="cb44-5"><a href="#cb44-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-6"><a href="#cb44-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Process the query.</span></span>
<span id="cb44-7"><a href="#cb44-7" aria-hidden="true" tabindex="-1"></a>query_results <span class="op">=</span> F.cosine_similarity(query_embedding, embeddings_db)</span>
<span id="cb44-8"><a href="#cb44-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-9"><a href="#cb44-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Examine results.</span></span>
<span id="cb44-10"><a href="#cb44-10" aria-hidden="true" tabindex="-1"></a>top_hit <span class="op">=</span> query_results.argsort(descending<span class="op">=</span><span class="va">True</span>)[<span class="dv">0</span>]</span>
<span id="cb44-11"><a href="#cb44-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-12"><a href="#cb44-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;[review #</span><span class="sc">{</span>top_hit<span class="sc">}</span><span class="ss">; score = </span><span class="sc">{</span>query_results[top_hit]<span class="sc">:.4f}</span><span class="ss">]</span><span class="ch">\n</span><span class="ss">&quot;</span>)</span>
<span id="cb44-13"><a href="#cb44-13" aria-hidden="true" tabindex="-1"></a><span class="co"># [review #17991; score = 0.7198]</span></span>
<span id="cb44-14"><a href="#cb44-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb44-15"><a href="#cb44-15" aria-hidden="true" tabindex="-1"></a>utils.print_wrapped(reviews[top_hit])</span>
<span id="cb44-16"><a href="#cb44-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Halloween is not only the godfather of all slasher movies but the greatest horror movie</span></span>
<span id="cb44-17"><a href="#cb44-17" aria-hidden="true" tabindex="-1"></a><span class="co"># ever! John Carpenter and Debra Hill created the most suspenseful, creepy, and terrifying</span></span>
<span id="cb44-18"><a href="#cb44-18" aria-hidden="true" tabindex="-1"></a><span class="co"># movie of all time with this classic chiller. Michael Myers is such a phenomenal monster</span></span>
<span id="cb44-19"><a href="#cb44-19" aria-hidden="true" tabindex="-1"></a><span class="co"># in this movie that he inspired scores of imitators, such as Jason Vorhees (Friday the</span></span>
<span id="cb44-20"><a href="#cb44-20" aria-hidden="true" tabindex="-1"></a><span class="co"># 13th), The Miner (My Bloody Valentine), and Charlie Puckett (The Night Brings Charlie).</span></span>
<span id="cb44-21"><a href="#cb44-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Okay, so I got a little obscure there, but it just goes to show you the impact that this</span></span>
<span id="cb44-22"><a href="#cb44-22" aria-hidden="true" tabindex="-1"></a><span class="co"># movie had on the entire horror genre.</span></span></code></pre></div>
</section>
<section id="sentiment-classification" class="slide level3">
<h3>Sentiment Classification</h3>
<div class="sourceCode" id="cb45"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SentimentClassificationTransformer(tfr.NextWordPredictionTransformer):</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Adapting a generative model to yield text embeddings.&quot;&quot;&quot;</span></span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>,</span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a>            pre_trained_model: tfr.NextWordPredictionTransformer,</span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a>            freeze_pre_trained: <span class="bu">bool</span> <span class="op">=</span> <span class="va">True</span></span>
<span id="cb45-8"><a href="#cb45-8" aria-hidden="true" tabindex="-1"></a>    ):</span>
<span id="cb45-9"><a href="#cb45-9" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(</span>
<span id="cb45-10"><a href="#cb45-10" aria-hidden="true" tabindex="-1"></a>            pre_trained_model._size_vocab,</span>
<span id="cb45-11"><a href="#cb45-11" aria-hidden="true" tabindex="-1"></a>            pre_trained_model._size_embed,</span>
<span id="cb45-12"><a href="#cb45-12" aria-hidden="true" tabindex="-1"></a>            pre_trained_model._n_heads,</span>
<span id="cb45-13"><a href="#cb45-13" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb45-14"><a href="#cb45-14" aria-hidden="true" tabindex="-1"></a>        <span class="kw">del</span> <span class="va">self</span>._linear</span>
<span id="cb45-15"><a href="#cb45-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.load_state_dict(pre_trained_model.state_dict(), strict<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb45-16"><a href="#cb45-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._logit <span class="op">=</span> nn.Linear(pre_trained_model._size_embed, <span class="dv">1</span>)</span>
<span id="cb45-17"><a href="#cb45-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-18"><a href="#cb45-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> freeze_pre_trained:</span>
<span id="cb45-19"><a href="#cb45-19" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> p <span class="kw">in</span> chain(<span class="va">self</span>._embedding.parameters(), <span class="va">self</span>._decoder.parameters()):</span>
<span id="cb45-20"><a href="#cb45-20" aria-hidden="true" tabindex="-1"></a>                p.requires_grad <span class="op">=</span> <span class="va">False</span></span>
<span id="cb45-21"><a href="#cb45-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-22"><a href="#cb45-22" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x: torch.Tensor) <span class="op">-&gt;</span> torch.Tensor:</span>
<span id="cb45-23"><a href="#cb45-23" aria-hidden="true" tabindex="-1"></a>        x_causal_mask, x_padding_mask <span class="op">=</span> <span class="va">self</span>._make_mask(x)</span>
<span id="cb45-24"><a href="#cb45-24" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>._embedding(x) <span class="op">*</span> math.sqrt(torch.tensor(<span class="va">self</span>._size_embed))</span>
<span id="cb45-25"><a href="#cb45-25" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>._position_encoder(out)</span>
<span id="cb45-26"><a href="#cb45-26" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>._decoder(</span>
<span id="cb45-27"><a href="#cb45-27" aria-hidden="true" tabindex="-1"></a>            out,</span>
<span id="cb45-28"><a href="#cb45-28" aria-hidden="true" tabindex="-1"></a>            out,</span>
<span id="cb45-29"><a href="#cb45-29" aria-hidden="true" tabindex="-1"></a>            tgt_mask<span class="op">=</span>x_causal_mask,</span>
<span id="cb45-30"><a href="#cb45-30" aria-hidden="true" tabindex="-1"></a>            tgt_key_padding_mask<span class="op">=</span>x_padding_mask,</span>
<span id="cb45-31"><a href="#cb45-31" aria-hidden="true" tabindex="-1"></a>            memory_mask<span class="op">=</span>x_causal_mask,</span>
<span id="cb45-32"><a href="#cb45-32" aria-hidden="true" tabindex="-1"></a>            memory_key_padding_mask<span class="op">=</span>x_padding_mask,</span>
<span id="cb45-33"><a href="#cb45-33" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb45-34"><a href="#cb45-34" aria-hidden="true" tabindex="-1"></a>        out <span class="op">=</span> torch.<span class="bu">max</span>(out, dim<span class="op">=</span><span class="dv">1</span>).values</span>
<span id="cb45-35"><a href="#cb45-35" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> F.sigmoid(<span class="va">self</span>._logit(out))</span></code></pre></div>
</section>
<section class="slide level3">

<p>Train the model:</p>
<div class="sourceCode" id="cb46"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>MAX_EPOCHS <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>BATCH_SIZE <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a>MIN_SEQ_LEN <span class="op">=</span> <span class="dv">10</span></span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a>MAX_SEQ_LEN <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a>MIN_WORD_FREQ <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a>LEARNING_RATE <span class="op">=</span> <span class="fl">0.0001</span></span></code></pre></div>
</section>
<section class="slide level3">

<p><img data-src="images/clf_train_stats.png" style="width:50.0%" /></p>
<pre class="text"><code>epoch 1 training loss = 0.4990: 100%|██████████| 666/666 [01:55&lt;00:00,  5.79it/s]
epoch 2 training loss = 0.3885: 100%|██████████| 666/666 [01:36&lt;00:00,  6.88it/s]
epoch 3 training loss = 0.3720: 100%|██████████| 666/666 [01:39&lt;00:00,  6.71it/s]
epoch 4 training loss = 0.3598: 100%|██████████| 666/666 [01:37&lt;00:00,  6.83it/s]
epoch 5 training loss = 0.3489: 100%|██████████| 666/666 [01:40&lt;00:00,  6.64it/s]
epoch 6 training loss = 0.3366: 100%|██████████| 666/666 [01:39&lt;00:00,  6.71it/s]
epoch 7 training loss = 0.3281: 100%|██████████| 666/666 [01:37&lt;00:00,  6.84it/s]
epoch 8 training loss = 0.3177: 100%|██████████| 666/666 [01:37&lt;00:00,  6.82it/s]
epoch 9 training loss = 0.3062: 100%|██████████| 666/666 [01:39&lt;00:00,  6.66it/s]
epoch 10 training loss = 0.2988: 100%|██████████| 666/666 [01:36&lt;00:00,  6.90it/s]

best model:
|-- epoch: 9
|-- validation loss: 0.3843</code></pre>
<p>Note → training converged in ~ 15 minutes!</p>
</section>
<section class="slide level3">

<p>Now test the model:</p>
<div class="sourceCode" id="cb48"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>hits <span class="op">=</span> torch.tensor(<span class="fl">0.0</span>)</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> x_batch, y_batch <span class="kw">in</span> test_dl:</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a>    y_pred <span class="op">=</span> sentiment_cls(x_batch)</span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a>    hits <span class="op">+=</span> torch.<span class="bu">sum</span>(y_pred.<span class="bu">round</span>() <span class="op">==</span> y_batch)</span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a>accuracy <span class="op">=</span> hits.item() <span class="op">/</span> (BATCH_SIZE <span class="op">*</span> <span class="bu">len</span>(test_dl))</span>
<span id="cb48-7"><a href="#cb48-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;accuracy = </span><span class="sc">{</span>accuracy<span class="sc">:.1%}</span><span class="ss">&quot;</span>)</span>
<span id="cb48-8"><a href="#cb48-8" aria-hidden="true" tabindex="-1"></a><span class="co"># accuracy = 83.9%</span></span></code></pre></div>
</section></section>
<section id="final-thoughts" class="title-slide slide level2">
<h2>Final thoughts</h2>
<div>
<ul>
<li class="fragment">We have achieved AutoNLP!</li>
<li class="fragment">There is a lot of engineering involved.</li>
</ul>
</div>
</section>
    </div>
  </div>

  <script src="https://unpkg.com/reveal.js@^4//dist/reveal.js"></script>

  <!-- reveal.js plugins -->
  <script src="https://unpkg.com/reveal.js@^4//plugin/notes/notes.js"></script>
  <script src="https://unpkg.com/reveal.js@^4//plugin/search/search.js"></script>
  <script src="https://unpkg.com/reveal.js@^4//plugin/zoom/zoom.js"></script>
  <script src="https://unpkg.com/reveal.js@^4//plugin/math/math.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
        // Display controls in the bottom right corner
        controls: true,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: true,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'bottom-right',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: false,

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: false,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: true,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'default',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: true,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'slide',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'fade',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [
          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    </body>
</html>
