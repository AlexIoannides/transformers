<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="author" content="Alex Ioannides">
  <title>Transformers &amp; LLMs</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="https://unpkg.com/reveal.js@^4//dist/reset.css">
  <link rel="stylesheet" href="https://unpkg.com/reveal.js@^4//dist/reveal.css">
  <style>
    .reveal .sourceCode {  /* see #7635 */
      overflow: visible;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        background-color: #232629;
        color: #7a7c7d;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #7a7c7d;  padding-left: 4px; }
    div.sourceCode
      { color: #cfcfc2; background-color: #232629; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #cfcfc2; } /* Normal */
    code span.al { color: #95da4c; background-color: #4d1f24; font-weight: bold; } /* Alert */
    code span.an { color: #3f8058; } /* Annotation */
    code span.at { color: #2980b9; } /* Attribute */
    code span.bn { color: #f67400; } /* BaseN */
    code span.bu { color: #7f8c8d; } /* BuiltIn */
    code span.cf { color: #fdbc4b; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #3daee9; } /* Char */
    code span.cn { color: #27aeae; font-weight: bold; } /* Constant */
    code span.co { color: #7a7c7d; } /* Comment */
    code span.cv { color: #7f8c8d; } /* CommentVar */
    code span.do { color: #a43340; } /* Documentation */
    code span.dt { color: #2980b9; } /* DataType */
    code span.dv { color: #f67400; } /* DecVal */
    code span.er { color: #da4453; text-decoration: underline; } /* Error */
    code span.ex { color: #0099ff; font-weight: bold; } /* Extension */
    code span.fl { color: #f67400; } /* Float */
    code span.fu { color: #8e44ad; } /* Function */
    code span.im { color: #27ae60; } /* Import */
    code span.in { color: #c45b00; } /* Information */
    code span.kw { color: #cfcfc2; font-weight: bold; } /* Keyword */
    code span.op { color: #cfcfc2; } /* Operator */
    code span.ot { color: #27ae60; } /* Other */
    code span.pp { color: #27ae60; } /* Preprocessor */
    code span.re { color: #2980b9; background-color: #153042; } /* RegionMarker */
    code span.sc { color: #3daee9; } /* SpecialChar */
    code span.ss { color: #da4453; } /* SpecialString */
    code span.st { color: #f44f4f; } /* String */
    code span.va { color: #27aeae; } /* Variable */
    code span.vs { color: #da4453; } /* VerbatimString */
    code span.wa { color: #da4453; } /* Warning */
  </style>
  <link rel="stylesheet" href="https://unpkg.com/reveal.js@^4//dist/theme/solarized.css" id="theme">
  <link rel="stylesheet" href="custom.css"/>
</head>
<body>
  <div class="reveal">
    <div class="slides">

<section id="title-slide">
  <h1 class="title">Transformers &amp; LLMs</h1>
  <p class="author">Alex Ioannides</p>
  <p class="date">October 12th, 2023</p>
</section>

<section
id="the-story-of-one-mans-mission-not-to-get-left-behind-in-the-dust"
class="title-slide slide level2">
<h2>The story of one man‚Äôs mission not to get left behind in the
dust</h2>
<p>‚Äú<em>Don‚Äôt let the little fuckers generation gap you.</em>‚Äù</p>
<p>- William Gibson, Neuromancer</p>
</section>

<section id="what-im-intending-to-talk-about"
class="title-slide slide level2">
<h2>What I‚Äôm intending to talk about</h2>
<div>
<ol type="1">
<li class="fragment">The problem we‚Äôre trying to solve.</li>
<li class="fragment">How to compute multi-head attention.</li>
<li class="fragment">Transformers: encoders, decoders, and all
that.</li>
<li class="fragment">How I developed a generative LLM.</li>
<li class="fragment">Exciting things to try with this LLM.</li>
<li class="fragment">Conclusions (heavily opinionated).</li>
</ol>
</div>
</section>

<section id="before-we-get-started" class="title-slide slide level2">
<h2>Before we get started</h2>
<p>This presentation is based on the codebase at <a
href="https://github.com/AlexIoannides/transformers-gen-ai">github.com/AlexIoannides/transformer-gen-ai</a>.</p>
<p>I‚Äôm not going to assume you‚Äôve worked through it, but if you have and
there are questions you want to ask, then please do üôÇ</p>
</section>

<section>
<section id="the-problem-were-trying-to-solve"
class="title-slide slide level2">
<h2>The problem we‚Äôre trying to solve</h2>

</section>
<section class="slide level3">

<p><img data-src="images/paradigm.png" /></p>
</section>
<section class="slide level3">

<p>The role that attention plays in all this‚Ä¶</p>
<p><img data-src="images/attention.png" /></p>
</section></section>
<section>
<section id="how-to-compute-multi-head-attention"
class="title-slide slide level2">
<h2>How to compute multi-head attention</h2>

</section>
<section id="lets-start-with-self-attention" class="slide level3">
<h3>Let‚Äôs start with self-attention</h3>
</section>
<section class="slide level3">

<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>VOCAB_SIZE <span class="op">=</span> <span class="dv">20000</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>EMBEDDING_DIM <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Let&#39;s assume some tokenizer has tokenised our sentence.</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>tokenized_sentence <span class="op">=</span> torch.randint(<span class="dv">0</span>, vocab_size, <span class="dv">8</span>)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>n_tokens <span class="op">=</span> <span class="bu">len</span>(tokenized_sentence)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="co"># We then map from token to embeddings.</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>embedding_layer <span class="op">=</span> nn.Embedding(VOCAB_SIZE, EMBEDDING_DIM)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>embedded_tokens <span class="op">=</span> embedding_layer(tokenized_sentence)</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="co"># And compute self-attention weights.</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>attn_weights <span class="op">=</span> torch.empty(n_tokens, n_tokens)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_tokens):</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(n_tokens):</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>        attn_weights[i, j] <span class="op">=</span> torch.dot(embedded_tokens[i], embedded_tokens[j])</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Normalise the weights, so that they sun to 1.0.</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>attn_weights_norm <span class="op">=</span> F.softmax(attn_weights <span class="op">/</span> math.sqrt(EMBEDDING_DIM), dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a><span class="co"># And finally use the weights to compute context-aware embeddings.</span></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>context_weighted_embeddings <span class="op">=</span> torch.matmul(attn_weights_norm, embedded_tokens)</span></code></pre></div>
</section>
<section class="slide level3">

<p>More formally‚Ä¶</p>
<p><span class="math display">\[
\vec{x_{i}} \to \vec{z_{i}} = \sum_{j=1}^{N}{a_{ij} \times \vec{x_{j}}}
\]</span></p>
<p>i.e., we build new embeddings using semantic similarity to
selectively pool information from the original embeddings. Note, there
aren‚Äôt any attention-specific parameters that need to be learnt, only
the original embeddings. We‚Äôll come back to this later.</p>
</section>
<section id="time-and-causality" class="slide level3">
<h3>Time and causality</h3>
<p>In the current setup, the attention-modulated embedding at time <span
class="math inline">\(t_1\)</span> is a function of embeddings for
tokens that come after. If we want to develop generative models, then
this isn‚Äôt appropriate. A common solution is to use <strong>causal
masking</strong>.</p>
</section>
<section class="slide level3">

<p><img data-src="images/causal_mask.png" /></p>
<div class="sourceCode" id="cb2"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>causal_mask <span class="op">=</span> torch.triu(torch.full((n_tokens, n_tokens), <span class="va">True</span>), diagonal<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>causal_attn_weights <span class="op">=</span> attn_weights.masked_fill(causal_mask, <span class="op">-</span><span class="fl">1e10</span>)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>causal_attn_weights_norm <span class="op">=</span> F.softmax(causal_attn_weights <span class="op">/</span> math.sqrt(EMBEDDING_DIM), dim<span class="op">=</span><span class="dv">1</span>)</span></code></pre></div>
</section>
<section id="learning-how-to-attend" class="slide level3">
<h3>Learning how to attend</h3>
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Define three linear transformations.</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>u_q <span class="op">=</span> torch.rand(n_tokens, n_tokens)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>u_k <span class="op">=</span> torch.rand(n_tokens, n_tokens)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>u_v <span class="op">=</span> torch.rand(n_tokens, n_tokens)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Use these to transform the embedded tokens.</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>q <span class="op">=</span> torch.matmul(u_q, embedded_tokens)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>k <span class="op">=</span> torch.matmul(u_k, embedded_tokens)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>v <span class="op">=</span> torch.matmul(u_v, embedded_tokens)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="co"># And then re-work the computation of the attention weights.</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>attn_weights_param <span class="op">=</span> torch.empty(n_tokens, n_tokens)</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_tokens):</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(n_tokens):</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>        attn_weights_param[i, j] <span class="op">=</span> torch.dot(q[i], k[j])</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>attn_weights_param_norm <span class="op">=</span> F.softmax(</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>    attn_weights_param <span class="op">/</span> math.sqrt(EMBEDDING_DIM), dim<span class="op">=</span><span class="dv">1</span></span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>context_weighted_embeddings_param <span class="op">=</span> torch.matmul(attn_weights_param_norm, v)</span></code></pre></div>
<p>This is equivalent to passing <code>embedded_tokens</code> through
three separate linear network layers and using the outputs within the
self-attention mechanism.</p>
</section>
<section id="from-single-to-multiple-attention-heads"
class="slide level3">
<h3>From single to multiple attention heads</h3>
<p><img data-src="images/multi_head_attention.png" /></p>
</section>
<section class="slide level3">

<p>‚Äú<em>The ‚ÄòAttention is all you need‚Äô paper was written at a time when
the idea of factoring feature spaces into independent subspaces had been
shown to provide great benefits for computer vision models‚Ä¶ Multi-head
attention is simply the application of the same idea to
self-attention.</em>‚Äù</p>
<p>- Fran√ßois Chollet (the author of Keras)</p>
</section>
<section class="slide level3">

<p>We have now arrived at</p>
<p><code>torch.nn.MultiheadAttention</code></p>
</section></section>
<section>
<section id="transformers-encoders-decoders-and-all-that"
class="title-slide slide level2">
<h2>Transformers: encoders, decoders, and all that</h2>

</section>
<section class="slide level3">

<p>How do we arrive at</p>
<p><code>torch.nn.TransformerEncoderLayer</code>
<code>torch.nn.TransformerDecoderLayer</code></p>
<p>?</p>
</section>
<section class="slide level3">

<p>Encoder-decoder for seq-to-seq translation</p>
<p><img data-src="images/encoder_decoder.png" style="width:40.0%" /></p>
<p>- ‚Äú<em>Attention is all you Need</em>‚Äù, Vaswani et al.¬†(2017)</p>
</section>
<section class="slide level3">

<p>‚Äú<em>‚Ä¶ adding residual connections, adding normalization layers‚Äîall
of these are standard architecture patterns that one would be wise to
leverage in any complex model. Together, these bells and whistles form
the Transformer encoder‚Äîone of two critical parts that make up the
Transformer architecture</em>‚Äù</p>
<p>- Fran√ßois Chollet (the author of Keras)</p>
</section>
<section class="slide level3">

<p>‚Äú<em>We stare into the void where our math fails us and try to write
math papers anyway‚Ä¶ We could then turn to the deepness itself and prove
things about batch norm or dropout or whatever, but these just give us
some nonpredictive post hoc justifications‚Ä¶ deep learning seems to drive
people completely insane.</em>‚Äù</p>
<p>- <a href="https://argmin.substack.com/p/my-mathematical-mind">Ben
Recht</a> (Prof.¬†of Computer Sciences, UC Berkley)</p>
</section>
<section class="slide level3">

<p>‚Äú<em>Deep learning is like riding a bicycle - it‚Äôs not something you
prove the existence of, it‚Äôs just something you do.</em>‚Äù</p>
<p>- Dr Alex Ioannides (ML Engineering Chapter Lead, Lloyd‚Äôs Banking
Group)</p>
</section>
<section id="when-do-we-use-encoders-decoders-or-both"
class="slide level3">
<h3>When do we use encoders, decoders, or both?</h3>
<div>
<ul>
<li class="fragment"><strong>Encoder</strong>: pure embedding
models.</li>
<li class="fragment"><strong>Decoder</strong>: generative models.</li>
<li class="fragment"><strong>Encoder + Decoder</strong>:
sequence-to-sequence models.</li>
</ul>
</div>
</section></section>
<section>
<section id="how-i-developed-a-generative-llm"
class="title-slide slide level2">
<h2>How I developed a generative LLM</h2>
<!--
- the dataset
- preparing the data - tokenisation
- preparing the data - PyTorch DataLoaders
- an aside on GPUs
- benchmark - training an RNN.
- using a generative model to generate text, given a prompt. 
- training a transformer-decoder.
    - positional encoding
    - learning rate schedule
-->
</section>
<section id="the-data" class="slide level3">
<h3>The data</h3>
<p>50k film reviews and sentiment scores from IMDB.</p>
<div class="sourceCode" id="cb4"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.nn.utils.rnn <span class="im">import</span> pad_sequence</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader, IterableDataset</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchtext.datasets <span class="im">import</span> IMDB</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchtext.vocab <span class="im">import</span> vocab</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> modelling.data <span class="im">import</span> (</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    FilmReviewSequences,</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    IMDBTokenizer,</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    get_data,</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>    make_chunks,</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>    make_sequence_datasets,</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>    pad_seq2seq_data,</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> get_data()</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>data.head(<span class="dv">10</span>)</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a><span class="co"># sentiment review</span></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a><span class="co"># 0 0   Forget what I said about Emeril. Rachael Ray i...</span></span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a><span class="co"># 1 0   Former private eye-turned-security guard ditch...</span></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a><span class="co"># 2 0   Mann photographs the Alberta Rocky Mountains i...</span></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a><span class="co"># 3 0   Simply put: the movie is boring. Clich√© upon c...</span></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a><span class="co"># 4 1   Now being a fan of sci fi, the trailer for thi...</span></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a><span class="co"># 5 1   In &#39;Hoot&#39; Logan Lerman plays Roy Eberhardt, th...</span></span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a><span class="co"># 6 0   This is the worst film I have ever seen.I was ...</span></span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a><span class="co"># 7 1   I think that Toy Soldiers is an excellent movi...</span></span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a><span class="co"># 8 0   I think Micheal Ironsides acting career must b...</span></span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a><span class="co"># 9 0   This was a disgrace to the game FarCry i had m...</span></span></code></pre></div>
</section>
<section class="slide level3">

<p>Example #4 in full:</p>
<p><em>‚ÄúNow being a fan of sci fi, the trailer for this film looked a
bit too, how do i put it, hollywood. But after watching it i can gladly
say it has impressed me greatly. Jude is a class actor and miss Leigh
pulls it off better than she did in Delores Clairborne. It brings films
like The Matrix, 12 Monkeys and The Cell into mind, which might not
sound that appealing, but it truly is one of the best films i have
seen.‚Äù</em></p>
</section>
<section id="generating-tokens" class="slide level3">
<h3>Generating tokens</h3>
<div class="sourceCode" id="cb5"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> IMDBTokenizer(_Tokenizer):</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Word to integer tokenization for use with any dataset or model.&quot;&quot;&quot;</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, reviews: <span class="bu">list</span>[<span class="bu">str</span>], min_word_freq: <span class="bu">int</span> <span class="op">=</span> <span class="dv">1</span>):</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>        reviews_doc <span class="op">=</span> <span class="st">&quot; &quot;</span>.join(reviews)</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>        token_counter <span class="op">=</span> Counter(<span class="va">self</span>._tokenize(reviews_doc))</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>        token_freqs <span class="op">=</span> <span class="bu">sorted</span>(token_counter.items(), key<span class="op">=</span><span class="kw">lambda</span> e: e[<span class="dv">1</span>], reverse<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>        _vocab <span class="op">=</span> vocab(OrderedDict(token_freqs), min_freq<span class="op">=</span>min_word_freq)</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>        _vocab.insert_token(<span class="st">&quot;&lt;pad&gt;&quot;</span>, PAD_TOKEN_IDX)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>        _vocab.insert_token(<span class="st">&quot;&lt;unk&gt;&quot;</span>, UNKOWN_TOKEN_IDX)</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>        _vocab.set_default_index(<span class="dv">1</span>)</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.vocab <span class="op">=</span> _vocab</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.vocab_size <span class="op">=</span> <span class="bu">len</span>(<span class="va">self</span>.vocab)</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> text2tokens(<span class="va">self</span>, text: <span class="bu">str</span>) <span class="op">-&gt;</span> <span class="bu">list</span>[<span class="bu">int</span>]:</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.vocab(<span class="va">self</span>._tokenize(text))</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> tokens2text(<span class="va">self</span>, tokens: <span class="bu">list</span>[<span class="bu">int</span>]) <span class="op">-&gt;</span> <span class="bu">str</span>:</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>        text <span class="op">=</span> <span class="st">&quot; &quot;</span>.join(<span class="va">self</span>.vocab.lookup_tokens(tokens))</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>        text <span class="op">=</span> re.sub(<span class="vs">rf&quot;\s</span><span class="sc">{</span>EOS_TOKEN<span class="sc">}</span><span class="vs">&quot;</span>, <span class="st">&quot;.&quot;</span>, text)</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> text.strip()</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>    <span class="at">@staticmethod</span></span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _tokenize(text: <span class="bu">str</span>) <span class="op">-&gt;</span> <span class="bu">list</span>[<span class="bu">str</span>]:</span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>        text <span class="op">=</span> IMDBTokenizer._standardise(text)</span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>        text <span class="op">=</span> (<span class="st">&quot;. &quot;</span>.join(sentence.strip() <span class="cf">for</span> sentence <span class="kw">in</span> text.split(<span class="st">&quot;.&quot;</span>))).strip()</span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>        text <span class="op">=</span> re.sub(<span class="vs">r&quot;\.&quot;</span>, <span class="ss">f&quot; </span><span class="sc">{</span>EOS_TOKEN<span class="sc">}</span><span class="ss"> &quot;</span>, text)</span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>        text <span class="op">=</span> re.sub(<span class="vs">r&quot;\s+&quot;</span>, <span class="st">&quot; &quot;</span>, text)</span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> text.split()</span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>    ...</span></code></pre></div>
</section>
<section class="slide level3">

<div class="sourceCode" id="cb6"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> IMDBTokenizer(_Tokenizer):</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Word to integer tokenization for use with any dataset or model.&quot;&quot;&quot;</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>    ...</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">@staticmethod</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _standardise(text: <span class="bu">str</span>) <span class="op">-&gt;</span> <span class="bu">str</span>:</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>        <span class="co">&quot;&quot;&quot;Remove punctuation, HTML and make lower case.&quot;&quot;&quot;</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>        text <span class="op">=</span> text.lower().strip()</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>        text <span class="op">=</span> unidecode(text)</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>        text <span class="op">=</span> re.sub(<span class="vs">r&quot;&lt;[^&gt;]*&gt;&quot;</span>, <span class="st">&quot;&quot;</span>, text)</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>        text <span class="op">=</span> re.sub(<span class="vs">r&quot;mr.&quot;</span>, <span class="st">&quot;mr&quot;</span>, text)</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>        text <span class="op">=</span> re.sub(<span class="vs">r&quot;mrs.&quot;</span>, <span class="st">&quot;mrs&quot;</span>, text)</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>        text <span class="op">=</span> re.sub(<span class="vs">r&quot;ms.&quot;</span>, <span class="st">&quot;ms&quot;</span>, text)</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>        text <span class="op">=</span> re.sub(<span class="vs">r&quot;(\!|\?)&quot;</span>, <span class="st">&quot;.&quot;</span>, text)</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>        text <span class="op">=</span> re.sub(<span class="vs">r&quot;-&quot;</span>, <span class="st">&quot; &quot;</span>, text)</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>        text <span class="op">=</span> <span class="st">&quot;&quot;</span>.join(</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>            char <span class="cf">for</span> char <span class="kw">in</span> text <span class="cf">if</span> char <span class="kw">not</span> <span class="kw">in</span> <span class="st">&quot;</span><span class="ch">\&quot;</span><span class="st">#$%&amp;&#39;()*+,/:;&lt;=&gt;@[</span><span class="ch">\\</span><span class="st">]^_`{|}~&quot;</span></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>        text <span class="op">=</span> re.sub(<span class="vs">r&quot;\.+&quot;</span>, <span class="st">&quot;.&quot;</span>, text)</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> text</span></code></pre></div>
</section>
<section class="slide level3">

<p><code>IMDBTokenizer</code> in action</p>
<div class="sourceCode" id="cb7"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>reviews <span class="op">=</span> data[<span class="st">&quot;review&quot;</span>].tolist()</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>review <span class="op">=</span> reviews[<span class="dv">0</span>]</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> IMDBTokenizer(reviews)</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>tokenized_review <span class="op">=</span> tokenizer(review)</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>tokenised_review_decoded <span class="op">=</span> tokenizer.tokens2text(tokenized_review[:<span class="dv">10</span>])</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;ORIGINAL TEXT: </span><span class="sc">{</span>review[:<span class="dv">47</span>]<span class="sc">}</span><span class="ss"> ...&quot;</span>)</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;TOKENS FROM TEXT: </span><span class="sc">{</span><span class="st">&#39;, &#39;</span><span class="sc">.</span>join(<span class="bu">str</span>(t) <span class="cf">for</span> t <span class="kw">in</span> tokenized_review[:<span class="dv">10</span>])<span class="sc">}</span><span class="ss"> ...&quot;</span>)</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;TEXT FROM TOKENS: </span><span class="sc">{</span>tokenised_review_decoded<span class="sc">}</span><span class="ss"> ...&quot;</span>)</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a><span class="co"># ORIGINAL TEXT: Forget what I said about Emeril. Rachael Ray is ...</span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a><span class="co"># TOKENS FROM TEXT: 831, 49, 11, 300, 44, 37877, 3, 10505, 1363, 8 ...</span></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a><span class="co"># TEXT FROM TOKENS: forget what i said about emeril. rachael ray is ...</span></span></code></pre></div>
<p>This is adequate for the current endeavor, but serious models use
more sophisticated tokenisation algorithms, such as <a
href="https://huggingface.co/learn/nlp-course/chapter6/5?fw=pt">Byte-Pair
Encoding (BPE)</a>, which is one of the ‚Äòsecret ingredients‚Äô of OpenAI‚Äôs
GPT models.</p>
</section>
<section id="datasets-and-dataloaders" class="slide level3">
<h3>Datasets and DataLoaders</h3>
<p>We would benefit from standardized interface for delivering data to
our models during training, which in this case requires two token
sequences (offset by one as we are developing generative models).
PyTorch provides such an interface:</p>
<p><code>torch.utils.data.IterableDataset</code></p>
<div class="sourceCode" id="cb8"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>tokenized_reviews <span class="op">=</span> [tokenizer(review) <span class="cf">for</span> review <span class="kw">in</span> reviews]</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> FilmReviewSequences(tokenized_reviews)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>x, y <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(dataset))</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;x[:5]: </span><span class="sc">{</span>x[:<span class="dv">5</span>]<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;y[:5]: </span><span class="sc">{</span>y[:<span class="dv">5</span>]<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a><span class="co"># x[:5]: tensor([831,  49,  11, 300,  44])</span></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a><span class="co"># y[:5]: tensor([   49,    11,   300,    44, 37877])</span></span></code></pre></div>
</section>
<section id="chunking" class="slide level3">
<h3>Chunking</h3>
<p>Most reviews are too long to be used as one input sequence and need
to be broken into chunks. I chose a strategy based on preserving
sentence integrity to create overlapping chunks that fall within a
maximum sequence length.</p>
</section>
<section class="slide level3">

<p>Example with maximum sequence length of 30 words:</p>
<div class="sourceCode" id="cb9"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>full_text <span class="op">=</span> <span class="st">&quot;&quot;&quot;I&#39;ve seen things you people wouldn&#39;t believe. Attack ships on fire off</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="st">the shoulder of Orion. I watched C-beams glitter in the dark near the Tannh√§user Gate.</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="st">All those moments will be lost in time, like tears in rain.&quot;&quot;&quot;</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>chunk_one <span class="op">=</span> <span class="st">&quot;&quot;&quot;I&#39;ve seen things you people wouldn&#39;t believe. Attack ships on fire off</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="st">the shoulder of Orion. I watched C-beams glitter in the dark near the Tannh√§user Gate.&quot;&quot;&quot;</span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>chunk_three <span class="op">=</span> <span class="st">&quot;&quot;&quot;Attack ships on fire off the shoulder of Orion. I watched C-beams</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a><span class="st">glitter in the dark near the Tannh√§user Gate.&quot;&quot;&quot;</span></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>chunk_four <span class="op">=</span> <span class="st">&quot;&quot;&quot;I watched C-beams glitter in the dark near the Tannh√§user Gate. All</span></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a><span class="st">those moments will be lost in time, like tears in rain.&quot;&quot;&quot;</span></span></code></pre></div>
</section>
<section class="slide level3">

<div class="sourceCode" id="cb10"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> FilmReviewSequences(IterableDataset):</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;IMDB film reviews for training generative models.&quot;&quot;&quot;</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>,</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>        tokenized_reviews: Iterable[<span class="bu">list</span>[<span class="bu">int</span>]],</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>        max_seq_len: <span class="bu">int</span> <span class="op">=</span> <span class="dv">40</span>,</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>        min_seq_len: <span class="bu">int</span> <span class="op">=</span> <span class="dv">20</span>,</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>        chunk_eos_token: <span class="bu">int</span> <span class="op">|</span> <span class="va">None</span> <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>        chunk_overlap: <span class="bu">bool</span> <span class="op">=</span> <span class="va">True</span>,</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>        tag: <span class="bu">str</span> <span class="op">=</span> <span class="st">&quot;data&quot;</span>,</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>    ):</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._data_file_path <span class="op">=</span> TORCH_DATA_STORAGE_PATH <span class="op">/</span> <span class="ss">f&quot;imdb_sequences_</span><span class="sc">{</span>tag<span class="sc">}</span><span class="ss">.json&quot;</span></span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> <span class="bu">open</span>(<span class="va">self</span>._data_file_path, mode<span class="op">=</span><span class="st">&quot;w&quot;</span>) <span class="im">as</span> <span class="bu">file</span>:</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> chunk_eos_token:</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> tok_review <span class="kw">in</span> tokenized_reviews:</span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>                    tok_chunks_itr <span class="op">=</span> make_chunks(</span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>                        tok_review,</span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>                        chunk_eos_token,</span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>                        max_seq_len,</span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>                        min_seq_len,</span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a>                        chunk_overlap</span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a>                    )</span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">for</span> tok_chunk <span class="kw">in</span> tok_chunks_itr:</span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a>                        <span class="bu">file</span>.write(json.dumps(tok_chunk) <span class="op">+</span> <span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>)</span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> tok_review <span class="kw">in</span> tokenized_reviews:</span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a>                    <span class="bu">file</span>.write(json.dumps(tok_review[:max_seq_len]) <span class="op">+</span> <span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>)</span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a>    ...</span></code></pre></div>
</section>
<section class="slide level3">

<div class="sourceCode" id="cb11"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> FilmReviewSequences(IterableDataset):</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;IMDB film reviews for training generative models.&quot;&quot;&quot;</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    ...</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__iter__</span>(<span class="va">self</span>) <span class="op">-&gt;</span> Iterable[<span class="bu">tuple</span>[Tensor, Tensor]]:</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> <span class="bu">open</span>(<span class="va">self</span>._data_file_path) <span class="im">as</span> <span class="bu">file</span>:</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> line <span class="kw">in</span> <span class="bu">file</span>:</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>                tokenized_chunk <span class="op">=</span> json.loads(line)</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>                <span class="cf">yield</span> (tensor(tokenized_chunk[:<span class="op">-</span><span class="dv">1</span>]), tensor(tokenized_chunk[<span class="dv">1</span>:]))</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>) <span class="op">-&gt;</span> <span class="bu">int</span>:</span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> <span class="bu">open</span>(<span class="va">self</span>._data_file_path) <span class="im">as</span> <span class="bu">file</span>:</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>            num_rows <span class="op">=</span> <span class="bu">sum</span>(<span class="dv">1</span> <span class="cf">for</span> line <span class="kw">in</span> <span class="bu">file</span>)</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> num_rows</span></code></pre></div>
<p>Note, the entire dataset it not held in memory, but is loaded from
disk on-demand in an attempt to optimize memory during training.</p>
</section>
<section class="slide level3">

<p>Use <code>DataLoader</code> to batch data and handle parallelism.</p>
<div class="sourceCode" id="cb12"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> pad_seq2seq_data(batch: <span class="bu">list</span>[<span class="bu">tuple</span>[<span class="bu">int</span>, <span class="bu">int</span>]]) <span class="op">-&gt;</span> <span class="bu">tuple</span>[Tensor, Tensor]:</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Pad sequence2sequence data tuples.&quot;&quot;&quot;</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> [e[<span class="dv">0</span>] <span class="cf">for</span> e <span class="kw">in</span> batch]</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> [e[<span class="dv">1</span>] <span class="cf">for</span> e <span class="kw">in</span> batch]</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>    x_padded <span class="op">=</span> pad_sequence(x, batch_first<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>    y_padded <span class="op">=</span> pad_sequence(y, batch_first<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x_padded, y_padded</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>data_loader <span class="op">=</span> DataLoader(datasets.test_data, batch_size<span class="op">=</span><span class="dv">10</span>, collate_fn<span class="op">=</span>pad_seq2seq_data)</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>data_batches <span class="op">=</span> [batch <span class="cf">for</span> batch <span class="kw">in</span> data_loader]</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>x_batch, y_batch <span class="op">=</span> data_batches[<span class="dv">0</span>]</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;x_batch_size = </span><span class="sc">{</span>x_batch<span class="sc">.</span>size()<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;y_batch_size = </span><span class="sc">{</span>y_batch<span class="sc">.</span>size()<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a><span class="co"># x_batch_size = torch.Size([10, 38])</span></span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a><span class="co"># y_batch_size = torch.Size([10, 38])</span></span></code></pre></div>
</section>
<section id="gpus" class="slide level3">
<h3>GPUs</h3>
<p>Basic approach - use the best available device for a given model.
Note that sometimes <code>mps</code> is slower than <code>cpu</code>
(until Apple get their act together).</p>
<div class="sourceCode" id="cb13"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch <span class="im">import</span> device</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_best_device(</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>        cuda_priority: Literal[<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>] <span class="op">=</span> <span class="dv">1</span>,</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>        mps_priority: Literal[<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>] <span class="op">=</span> <span class="dv">2</span>,</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>        cpu_priority: Literal[<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>] <span class="op">=</span> <span class="dv">3</span>,</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>    ) <span class="op">-&gt;</span> device:</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>    <span class="co">&quot;&quot;&quot;Return the best device available on the machine.&quot;&quot;&quot;</span></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>    device_priorities <span class="op">=</span> <span class="bu">sorted</span>(</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>        ((<span class="st">&quot;cuda&quot;</span>, cuda_priority), (<span class="st">&quot;mps&quot;</span>, mps_priority), (<span class="st">&quot;cpu&quot;</span>, cpu_priority)),</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>        key<span class="op">=</span><span class="kw">lambda</span> e: e[<span class="dv">1</span>]</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> device_type, _ <span class="kw">in</span> device_priorities:</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> device_type <span class="op">==</span> <span class="st">&quot;cuda&quot;</span> <span class="kw">and</span> cuda.is_available():</span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> device(<span class="st">&quot;cuda&quot;</span>)</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> device_type <span class="op">==</span> <span class="st">&quot;mps&quot;</span> <span class="kw">and</span> mps.is_available():</span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> device(<span class="st">&quot;mps&quot;</span>)</span>
<span id="cb13-19"><a href="#cb13-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> device_type <span class="op">==</span> <span class="st">&quot;cpu&quot;</span>:</span>
<span id="cb13-20"><a href="#cb13-20" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> device(<span class="st">&quot;cpu&quot;</span>)</span></code></pre></div>
</section>
<section class="slide level3">

<p>Models were trained using one of:</p>
<ul>
<li>Apple M1 Max</li>
<li>AWS <code>p3.xlarge</code> instance with a single NVIDIA V100 Tensor
Core GPU.</li>
</ul>
</section></section>
<section id="benchmarking-with-an-rnn" class="title-slide slide level2">
<h2>Benchmarking with an RNN</h2>
<p>TODO</p>
</section>

<section id="generative-transformer-decoder"
class="title-slide slide level2">
<h2>Generative transformer-decoder</h2>
<p>TODO</p>
</section>

<section id="exciting-things-to-try-with-this-llm"
class="title-slide slide level2">
<h2>Exciting things to try with this LLM</h2>
<!--
- semantic search
- sentiment classification
-->
</section>

<section id="conclusions-heavily-opinionated"
class="title-slide slide level2">
<h2>Conclusions (heavily opinionated)</h2>
<!--
- We have achieved full AutoNLP.
- This is alchemy, not physics.
- Emergent capabilities is probably a fallacy.
- But it has been shown that deep learning can grok...
- ... but for now probably just VERY useful stochastic parrots.
-->
</section>
    </div>
  </div>

  <script src="https://unpkg.com/reveal.js@^4//dist/reveal.js"></script>

  <!-- reveal.js plugins -->
  <script src="https://unpkg.com/reveal.js@^4//plugin/notes/notes.js"></script>
  <script src="https://unpkg.com/reveal.js@^4//plugin/search/search.js"></script>
  <script src="https://unpkg.com/reveal.js@^4//plugin/zoom/zoom.js"></script>
  <script src="https://unpkg.com/reveal.js@^4//plugin/math/math.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
        // Display controls in the bottom right corner
        controls: true,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: true,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'bottom-right',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: false,

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: false,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: true,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'default',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: true,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'slide',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'fade',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [
          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    </body>
</html>
