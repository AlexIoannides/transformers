{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers for Search\n",
    "\n",
    "Let's take a look at how we can use language models to perform a simple search task - given 50,000 movie reviews, can we find one that describes a file that we might be interested in watching?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "We will lean heavily on our `modelling` package for loading pre-trained models and adapting them to the search task, for which we will also need to import PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from modelling import data, utils\n",
    "from modelling import transformer as tfr\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "There are three key parameters we need to set:\n",
    "\n",
    "- The number of text tokens from each review to use for creating embeddings (set to 60 as this matches what we used for training the model).\n",
    "\n",
    "- The name of the pre-trained transformer model to use as the foundation for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZE = 100\n",
    "MODEL_NAME = \"decoder_next_word_gen\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data\n",
    "\n",
    "Load the reviews into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forget what I said about Emeril. Rachael Ray is the most irritating personality on the\n",
      "Food Network AND all of television. If you've never seen 30 Minute Meals, then you cannot\n",
      "possibly begin to comprehend how unfathomably annoying she is. I really truly meant that\n",
      "you can't even begin to be boggled by her until you've viewed the show once or twice, and\n",
      "even then all words and intelligent thoughts will fail you. The problem is mostly with\n",
      "her mannerisms as you might have guessed. Ray has a goofy mouth and often imitates the\n",
      "parrot. If you love something or think it's \"awesome\" (a word she uses roughly 87 times\n",
      "per telecast) just say it. And she's constantly using horrible, unfunny catchphrases like\n",
      "\"EVOO\" (Extra virgin olive oil!). SHUT UP! What's worse is Ray has TWO other shows on the\n",
      "network! I think this is some elaborate conspiracy by the terrorists to drive us mad.\n",
      "Give me more Tyler Florence! Ray is lame.\n"
     ]
    }
   ],
   "source": [
    "reviews = data.get_data()[\"review\"].tolist()\n",
    "review = reviews[0]\n",
    "\n",
    "utils.print_wrapped(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Tokenizer\n",
    "\n",
    "We will need a tokenizer to convert reviews from strings to lists of integers, that the model has been trained to use as inputs. We will need to use the same tokenizer as that used for training the original model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[831, 49, 11, 300, 44, 1, 3, 10505, 1363, 8]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = data.IMDBTokenizer(reviews, 10)\n",
    "\n",
    "tokenizer(review)[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pre-Trained Model\n",
    "\n",
    "We load a pre-trained transformer decoder (generative) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading .models/decoder_next_word_gen/trained@2023-07-23T10:13:30;loss=5_0299.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NextWordPredictionTransformer(\n",
       "  (_position_encoder): PositionalEncoding(\n",
       "    (_dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (_embedding): Embedding(133046, 256)\n",
       "  (_decoder): TransformerDecoderLayer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "    )\n",
       "    (multihead_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.1, inplace=False)\n",
       "    (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    (dropout3): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (_linear): Linear(in_features=256, out_features=133046, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_trained_model: tfr.NextWordPredictionTransformer = utils.load_model(MODEL_NAME)\n",
    "pre_trained_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adapt Pre-Trained Model to Create Document Embeddings\n",
    "\n",
    "Recall that the pre-trained model was original trained to predict the next token in a sequence, which is ultimately a classification task that necessitated the final layer of the model being a linear layer that output the logits for all possible tokens (~30k). \n",
    "\n",
    "Well, we don't need this linear layer to create embeddings - we'd prefer to have the intermediate output from the core transformer block - i.e., the context-aware token embeddings output the from multi-head attention mechanism.\n",
    "\n",
    "To get at these we define a new model (using inheritance) that will only initialise and use the layers of the pre-trained model that we want. We also add an additional step that will map many context-aware embeddings to a single embedding for a whole chunk of text (or document)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentEmbeddingTransformer(tfr.NextWordPredictionTransformer):\n",
    "    \"\"\"Adapting a generative model to yield text embeddings.\"\"\"\n",
    "\n",
    "    def __init__(self, pre_trained_model: tfr.NextWordPredictionTransformer):\n",
    "        super().__init__(\n",
    "            pre_trained_model._size_vocab,\n",
    "            pre_trained_model._size_embed,\n",
    "            pre_trained_model._n_heads,\n",
    "        )\n",
    "        del self._linear\n",
    "        self.load_state_dict(pre_trained_model.state_dict(), strict=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x_causal_mask, x_padding_mask = self._make_mask(x)\n",
    "        out = self._embedding(x) * math.sqrt(torch.tensor(self._size_embed))\n",
    "        out = self._position_encoder(out)\n",
    "        out = self._decoder(\n",
    "            out,\n",
    "            out,\n",
    "            tgt_mask=x_causal_mask,\n",
    "            tgt_key_padding_mask=x_padding_mask,\n",
    "            memory_mask=x_causal_mask,\n",
    "            memory_key_padding_mask=x_padding_mask,\n",
    "        )\n",
    "        out = torch.sum(out.squeeze(), dim=0)\n",
    "        out /= out.norm()\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create an instance of our document embedding model and feed it a tokenised chunk of text to make sure that what we get back is a single vector with the same dimension as our context-aware embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_model = DocumentEmbeddingTransformer(pre_trained_model)\n",
    "embedding = embedding_model(torch.tensor([tokenizer(review)]))\n",
    "embedding.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index Reviews using Embeddings\n",
    "\n",
    "We now use our document embedding model to produce an embedding vector for each review in the dataset - all 50,000!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_db = []\n",
    "errors = []\n",
    "\n",
    "embedding_model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, review in enumerate(reviews):\n",
    "        try:\n",
    "            review_tokenized = tokenizer(reviews[i])[:CHUNK_SIZE]\n",
    "            review_embedding = embedding_model(torch.tensor([review_tokenized]))\n",
    "            embeddings_db.append(review_embedding)\n",
    "        except Exception:\n",
    "            errors.append(str(i))\n",
    "\n",
    "if errors:\n",
    "    print(f\"ERRORS: {', '.join(errors)}\")\n",
    "\n",
    "embeddings_db = torch.stack(embeddings_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Reviews\n",
    "\n",
    "We now have everything we need approach our search task. We start by specifying a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Classic horror movie that is terrifying\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create an embedding for our query and use [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) to score all reviews for relevance to our query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_embedding = embedding_model(torch.tensor([tokenizer(query)]))\n",
    "query_results = F.cosine_similarity(query_embedding, embeddings_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the top query result to see if it sounds like a relavent match for our query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[review #14212; score = 0.7540]\n",
      "\n",
      "\"Pet Sematary\" succeeds on two major situations. First, it's a scary Horror movie. Those\n",
      "that just aren't produced in these days. Second, it's an emotional, clever movie overall.\n",
      "So if you are looking for chills, scares, creepiness and visually stunning settings,\n",
      "great acting, dialongs, and gruesome effects; this is the movie you are looking for. A\n",
      "classic now and truly a must see for any Horror fan. <br /><br />Probably, the best\n",
      "adaptation to any of King's novels. The events feel a little rushed compared with the\n",
      "novel, but that doesn't means that this underrated movie isn't a complete Horror/Drama\n",
      "accomplishment. <br /><br />Stephen King's novel is widely known for being very emotional\n",
      "and gruesome at the same time. The movie captures the same feeling mainly because there's\n",
      "a great character development and you can feel the loving relationship between it's\n",
      "members. Then, when everything seems to be happiness (technically happy, because the\n",
      "title \"Pet Sematary\" does not offers appiness!) a tragic event changes the movie's\n",
      "atmosphere, now it turns very dark. The movie has a sinister feeling since the opening\n",
      "credits, but after Gage is killed the movie becomes sad, gray, creepy. Dealing with the\n",
      "loss of a baby son is something that can ruin a family's entire life, and \"Pet Sematary\"\n",
      "proves it dramatically. <br /><br />The legend behind the pet sematary is more than a\n",
      "myth that no one wants to experience, but sadness and desperation lead an emotionally\n",
      "destroyed father to give it a shot. Sadly enough, the legend comes true and baby Gage\n",
      "returns from the dead. The previous encounter with the pet sematary legend turned out to\n",
      "be a tragedy but this time it's something much, much worse. What will happened with the\n",
      "lives of our All American family? Could Pascow prevent this tragedy? What is it with the\n",
      "surreal nightmares? <br /><br />Watch \"Pet Sematary\" to witness one of the most touching,\n",
      "emotional Horror movies of recent times. You won't regret. The acting is very good\n",
      "although I didn't dig the actor who portrayed the father. He didn't seem disturbed enough\n",
      "when the situations asked for his desperation. But that's just my opinion. Denise Crosby\n",
      "truly delivered a great performance and worked perfect as the noble, tender mother. Baby\n",
      "Gage was amazing even on his creepy parts. *Shivers*. Overall this is a great classic of\n",
      "all time and a disturbing movie that touches people's deepest fears... the loss of\n",
      "someone you love, the dead returning to life, and a feeling of desperation.<br /><br\n",
      "/>Something is for sure... I don't wanna be buried, in a pet sematary!!\n"
     ]
    }
   ],
   "source": [
    "query_embedding = embedding_model(torch.tensor([tokenizer(query)]))\n",
    "query_results = F.cosine_similarity(query_embedding, embeddings_db)\n",
    "\n",
    "top_hit = query_results.argsort(descending=True)[0]\n",
    "\n",
    "print(f\"[review #{top_hit}; score = {query_results[top_hit]:.4f}]\\n\")\n",
    "utils.print_wrapped(reviews[top_hit])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad for a toy model!\n",
    "\n",
    "In practice, pre-trained language models are not used for semantic search tasks (i.e., for sentence embeddings), without having first fine-tuned them to ensure that performance for this task has been optimised. This is beyond the scope of this work, but the next logical step to try."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
