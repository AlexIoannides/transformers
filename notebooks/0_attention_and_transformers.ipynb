{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention and Transformers\n",
    "\n",
    "We will demonstrate how attention mechanisms work, how they can be implemented and their use within model based on transformer architectures. We will develop an understanding from first principles using PyTorch for creating and manipulating tensors.\n",
    "\n",
    "Ultimately, we're aiming to demystify what's' happening within PyTorch's high-level transformer modules: [torch.nn.TransformerEncoderLayer](https://pytorch.org/docs/stable/generated/torch.nn.TransformerEncoderLayer.html#torch.nn.TransformerEncoderLayer) and [torch.nn.TransformerDecoderLayer](https://pytorch.org/docs/stable/generated/torch.nn.TransformerDecoderLayer.html#torch.nn.TransformerDecoderLayer).\n",
    "\n",
    "Attention mechanisms aim to map [word embeddings](https://en.wikipedia.org/wiki/Word_embedding) from one vector space into another, based on the other word embeddings in the sequence. This produces context-aware embeddings.\n",
    "\n",
    "We could express this mapping mathematically as, $\\textbf{x} \\to \\textbf{z} = f(\\textbf{x})$, where $\\textbf{x} = (\\vec{x_{1}}, ..., \\vec{x_{N}})$, $\\textbf{z} = (\\vec{z_{1}}, ..., \\vec{z_{N}})$, $\\vec{x}$ and $\\vec{z}$ are $d$-dimensional embedding vectors and $N$ is the number of tokens in the sequence. The goal of attention is to learn $f$ from data to solve machine learning tasks such as sequence-to-sequence translation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "We will mostly be using PyTorch like NumPy (to create and manipulate tensors), but we will also use one or two modules from its neural networks module, `torch.nn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Sorry Dave, I'm afraid I can't do that.\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then [tokenize](https://en.wikipedia.org/wiki/Lexical_analysis#Tokenization) our sentence into a sequence of integer values (one for each word), using an imaginary tokenization function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10277, 18871, 14910, 13181,  2829, 19980,  9604, 10053])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize(text: str, vocab_size: int) -> torch.Tensor:\n",
    "    \"\"\"Dummy text tokenizer.\"\"\"\n",
    "    words = text.split(\" \")\n",
    "    return torch.randint(0, vocab_size, [len(words)])\n",
    "\n",
    "\n",
    "VOCAB_SIZE = 20000\n",
    "\n",
    "tokenized_sentence = tokenize(sentence, VOCAB_SIZE)\n",
    "n_tokens = len(tokenized_sentence)\n",
    "tokenized_sentence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And embed each token into a vector space using PyTorch's [torch.nn.Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding) module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 32])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EMBEDDING_DIM = 32\n",
    "\n",
    "embedding_layer = nn.Embedding(VOCAB_SIZE, EMBEDDING_DIM)\n",
    "embedded_tokens = embedding_layer(tokenized_sentence)\n",
    "embedded_tokens.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These embeddings will need to be learnt when training any model that uses an embedding layer. We can easily compute the number of parameters that need to be learnt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of embedding parameters = 640,000\n"
     ]
    }
   ],
   "source": [
    "n_embedding_params = sum(len(p.flatten()) for p in embedding_layer.parameters())\n",
    "print(f\"number of embedding parameters = {n_embedding_params:,}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Self-Attention"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An approach to computing attention is to express the new context-aware embeddings as a weighted linear combination or the input embeddings - e.g., $\\vec{x_{i}} \\to \\vec{z_{i}} = \\sum_{j=1}^{N}{a_{ij} \\times \\vec{x_{j}}}$. \n",
    "\n",
    "One sensible approach to computing the weights is to use the vector [dot product](https://en.wikipedia.org/wiki/Dot_product) between the embedding vectors - e.g., $a_{ij} = x_{i}^{T} \\cdot x_{i}$. This will lead to weights that are higher for embedding vectors that are geometrically nearer to one another in the embedding space (i.e., are semantically closer), and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights = torch.empty(n_tokens, n_tokens)\n",
    "\n",
    "for i in range(n_tokens):\n",
    "    for j in range(n_tokens):\n",
    "        attn_weights[i, j] = torch.dot(embedded_tokens[i], embedded_tokens[j])\n",
    "\n",
    "attn_weights.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This calculation can also be computed more efficiently using matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_weights_matmul = torch.matmul(embedded_tokens, embedded_tokens.T)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can verify that the two approaches are equivalent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(attn_weights_matmul, attn_weights, atol=1e-6)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When it comes to implementing this in practice the weights are scaled by the embedding dimension, and subsequently renormalised to sum to one across rows using the [softmax function](https://en.wikipedia.org/wiki/Softmax_function). Steps like these make models easier to train by normalising the magnitude of gradients used within algorithms like [stochastic gradient descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent). Refer to [3] for more insight into this and related issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_weights_norm = F.softmax(attn_weights / math.sqrt(EMBEDDING_DIM), dim=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that rows sum to one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "       grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights_norm.sum(dim=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compute the final context-aware embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 32])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_weighted_embeddings = torch.matmul(attn_weights_norm, embedded_tokens)\n",
    "context_weighted_embeddings.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify that these embeddings are working as we expect by computing one manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.7159,  0.4557,  0.8079, -0.3945, -0.3668,  0.9979, -0.1918, -1.1513,\n",
       "        -0.8344, -0.8734,  0.7873, -0.4425, -0.7375, -0.5568,  0.1270,  0.6518,\n",
       "        -0.1288, -0.5502,  0.5016,  0.0821, -0.1772,  1.1589,  0.9620,  2.0633,\n",
       "         1.4493,  0.5128, -0.0210, -0.4529,  0.0912, -0.0551,  0.1342, -0.4279],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_weighted_embeddings_3 = (\n",
    "    attn_weights_norm[3, 0] * embedded_tokens[0]\n",
    "    + attn_weights_norm[3, 1] * embedded_tokens[1]\n",
    "    + attn_weights_norm[3, 2] * embedded_tokens[2]\n",
    "    + attn_weights_norm[3, 3] * embedded_tokens[3]\n",
    "    + attn_weights_norm[3, 4] * embedded_tokens[4]\n",
    "    + attn_weights_norm[3, 5] * embedded_tokens[5]\n",
    "    + attn_weights_norm[3, 6] * embedded_tokens[6]\n",
    "    + attn_weights_norm[3, 7] * embedded_tokens[7]\n",
    ")\n",
    "\n",
    "context_weighted_embeddings_3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And verifying the output against the matrix multiplication computed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(context_weighted_embeddings_3, context_weighted_embeddings[3], atol=1e-6)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Causal Masking\n",
    "\n",
    "You may haven noticed that the embedding vector for the first word, $\\vec{x_{1}}$, is mapped to a vector $\\vec{z_{1}}$ that is a function of embedding vectors for words that come after the first word. This isn't a problem if all we're doing is creating embeddings (or sequences) based on whole passages of text. It does pose a problem, however, if we're trying to develop a model that can generate new sequences given an initial sequence (or prompt). This problem is solved by using causal masking.\n",
    "\n",
    "Causal masking matrices can be constructed to flag which attention weights should be set to zero so that causal relationships between embeddings aren't broken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False, False,  True,  True,  True,  True,  True],\n",
       "        [False, False, False, False,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False,  True,  True,  True],\n",
       "        [False, False, False, False, False, False,  True,  True],\n",
       "        [False, False, False, False, False, False, False,  True],\n",
       "        [False, False, False, False, False, False, False, False]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "causal_mask = torch.triu(torch.full((n_tokens, n_tokens), True), diagonal=1)\n",
    "causal_mask"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which we will apply directly to the attention weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4.3661e+01, -1.0000e+10, -1.0000e+10, -1.0000e+10, -1.0000e+10,\n",
       "        -1.0000e+10, -1.0000e+10, -1.0000e+10], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "causal_attn_weights = attn_weights.masked_fill(causal_mask, -1e10)\n",
    "causal_attn_weights[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And apply scaling and normalisation as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "       grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "causal_attn_weights_norm = F.softmax(\n",
    "    causal_attn_weights / math.sqrt(EMBEDDING_DIM), dim=1\n",
    ")\n",
    "causal_attn_weights_norm.sum(dim=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From which we can compute causal context-aware embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 32])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "causal_context_weighted_embeddings = torch.matmul(\n",
    "    causal_attn_weights_norm, embedded_tokens\n",
    ")\n",
    "causal_context_weighted_embeddings.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The integrity of the causal structure is easily demonstrated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "causal_context_weighted_embeddings_3 = (\n",
    "    causal_attn_weights_norm[3, 0] * embedded_tokens[0]\n",
    "    + causal_attn_weights_norm[3, 1] * embedded_tokens[1]\n",
    "    + causal_attn_weights_norm[3, 2] * embedded_tokens[2]\n",
    "    + causal_attn_weights_norm[3, 3] * embedded_tokens[3]\n",
    ")\n",
    "\n",
    "torch.allclose(\n",
    "    causal_context_weighted_embeddings_3, causal_context_weighted_embeddings[3]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parametrised Self-Attention\n",
    "\n",
    "Up to this point we have described a basic attention mechanism where the only parameters that can be learnt are for the initial embedding vectors. At this point the system is limited in its ability to adapt the attention mechanism to the task(s) at hand."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Queries, Keys and Values\n",
    "\n",
    "We begin by generalising the attention mechanism - let, $\\textbf{q} = (\\vec{q_{1}}, ..., \\vec{q_{N}})$, $\\textbf{k} = (\\vec{k_{1}}, ..., \\vec{k_{N}})$ and $\\textbf{v} = (\\vec{v_{1}}, ..., \\vec{v_{N}})$ be three new sequences representing a query, keys and values respectively. In this setup, the values contain the information that we wish to access via a query that is made on a set of keys (that map to the values), such that the context-aware embeddings can now be computed as,\n",
    "\n",
    "$$\n",
    "\\vec{z_{i}} = \\sum_{j=1}^{N}{a_{ij} \\times \\vec{v_{j}}}\n",
    "$$\n",
    "\n",
    "Where, $a_{ij} = q_{i}^{T} \\cdot k_{i}$ - i.e., the attention weights now represent the distance between the query and keys.\n",
    "\n",
    "Very often we only have a single sequence to work with, so the model will have to learn how to infer the queries, keys and values from this. We can enable this level of plasticity by defining three  $N \\times N$ weight matrices, $\\textbf{U}_{q}$, $\\textbf{U}_{k}$ and $\\textbf{U}_{v}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_q = torch.rand(n_tokens, n_tokens)\n",
    "u_k = torch.rand(n_tokens, n_tokens)\n",
    "u_v = torch.rand(n_tokens, n_tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From which we can define the query, keys and values as functions of $\\textbf{x}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = torch.matmul(u_q, embedded_tokens)\n",
    "k = torch.matmul(u_k, embedded_tokens)\n",
    "v = torch.matmul(u_v, embedded_tokens)\n",
    "\n",
    "q.shape == k.shape == v.shape == embedded_tokens.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then recompute our parameterised attention weights using the same steps we used before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 32])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights_param = torch.empty(n_tokens, n_tokens)\n",
    "\n",
    "for i in range(n_tokens):\n",
    "    for j in range(n_tokens):\n",
    "        attn_weights_param[i, j] = torch.dot(q[i], k[j])\n",
    "\n",
    "attn_weights_param_norm = F.softmax(\n",
    "    attn_weights_param / math.sqrt(EMBEDDING_DIM), dim=1\n",
    ")\n",
    "context_weighted_embeddings_param = torch.matmul(attn_weights_param_norm, v)\n",
    "\n",
    "context_weighted_embeddings_param.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And verify that the context-aware embeddings behave as we'd expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_weighted_embeddings_param_3 = (\n",
    "    attn_weights_param_norm[3, 0] * v[0]\n",
    "    + attn_weights_param_norm[3, 1] * v[1]\n",
    "    + attn_weights_param_norm[3, 2] * v[2]\n",
    "    + attn_weights_param_norm[3, 3] * v[3]\n",
    "    + attn_weights_param_norm[3, 4] * v[4]\n",
    "    + attn_weights_param_norm[3, 5] * v[5]\n",
    "    + attn_weights_param_norm[3, 6] * v[6]\n",
    "    + attn_weights_param_norm[3, 7] * v[7]\n",
    ")\n",
    "\n",
    "torch.allclose(\n",
    "    context_weighted_embeddings_param_3, context_weighted_embeddings_param[3]\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Head Attention\n",
    "\n",
    "<center><img src=\"images/attention.png\" width=\"500\"/></center>\n",
    "\n",
    "In what follows we demonstrate how use the parametrised attention mechanism sketched out above to develop the multi-head attention block that forms the foundation of all transformer architectures. Our aim here is purely didactic - the functions defined below won't yield anything you can train (refer to the full codebase in the `modelling` directory for this), but they do demonstrate how these algorithm are composed.\n",
    "\n",
    "We start by encapsulating the parametrised attention mechanism within a single function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 32])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def attention(\n",
    "    query: torch.Tensor,\n",
    "    keys: torch.Tensor,\n",
    "    values: torch.Tensor,\n",
    "    causal_masking: bool = False,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Compute single attention head.\"\"\"\n",
    "    n_tokens, embedding_dim = query.shape\n",
    "    attn_weights = torch.matmul(query, keys.T) / math.sqrt(EMBEDDING_DIM)\n",
    "    if causal_masking:\n",
    "        mask = torch.triu(torch.full((n_tokens, n_tokens), True), diagonal=1)\n",
    "        attn_weights = attn_weights.masked_fill(mask, -1e10)\n",
    "    attn_weights_norm = attn_weights.softmax(dim=1)\n",
    "    context_weighted_embeddings = torch.matmul(attn_weights_norm, values)\n",
    "    return context_weighted_embeddings\n",
    "\n",
    "\n",
    "attn_head_out = attention(q, k, v)\n",
    "attn_head_out.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And use this to define an attention mechanism with multiple attention blocks or 'heads'. This enables models to learn multiple 'contexts' - different sets of keys and values - not unlike how convolutional neural networks use multiple sets of filter banks to detect features at different scales (it is likely that this analog is what motivated this design)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 32])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def multi_head_attention(\n",
    "    x_q: torch.Tensor,\n",
    "    x_k: torch.Tensor,\n",
    "    x_v: torch.Tensor,\n",
    "    n_heads: int,\n",
    "    causal_masking: bool = False,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Computing attention with multiple heads.\"\"\"\n",
    "    n_tokens, embedding_dim = embedded_tokens.shape\n",
    "\n",
    "    u_q = torch.rand(n_heads, n_tokens, n_tokens)\n",
    "    u_k = torch.rand(n_heads, n_tokens, n_tokens)\n",
    "    u_v = torch.rand(n_heads, n_tokens, n_tokens)\n",
    "\n",
    "    attn_head_outputs = torch.concat(\n",
    "        [attention(u_q[h] @ x_q, u_k[h] @ x_k, u_v[h] @ x_v) for h in range(n_heads)],\n",
    "        dim=1,\n",
    "    )\n",
    "\n",
    "    w_out = torch.rand(n_heads * embedding_dim, embedding_dim, requires_grad=True)\n",
    "    return torch.matmul(attn_head_outputs, w_out)\n",
    "\n",
    "\n",
    "multi_head_attn_out = multi_head_attention(\n",
    "    embedded_tokens, embedded_tokens, embedded_tokens, n_heads=3\n",
    ")\n",
    "multi_head_attn_out.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, that `@` is a shorthand operator for matrix multiplication and that `torch.rand(n_heads, n_tokens, n_tokens)` could also be replaced with `nn.Linear` as these matrices are equivalent to passing the inputs through a fully-connected (or dense) network layer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers\n",
    "\n",
    "<center><img src=\"images/encoder_decoder.png\" width=\"500\"/></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now know enough to assemble the basic transformer architecture, starting with a single layer encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 32])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def transformer_encoder_layer(\n",
    "    src_embedding: torch.Tensor, n_heads: int, causal_masking: bool = False\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Transformer encoder layer.\"\"\"\n",
    "    x = multi_head_attention(src_embedding, src_embedding, src_embedding, n_heads)\n",
    "    x = F.layer_norm(x + src_embedding, x.shape)\n",
    "\n",
    "    linear_1 = nn.Linear(EMBEDDING_DIM, 2 * EMBEDDING_DIM)\n",
    "    linear_2 = nn.Linear(2 * EMBEDDING_DIM, EMBEDDING_DIM)\n",
    "\n",
    "    x = x + F.relu(linear_2(linear_1(x)))\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "encoder_output = transformer_encoder_layer(embedded_tokens, n_heads=2)\n",
    "encoder_output.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then a single layer decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 32])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def transformer_decoder_layer(\n",
    "    src_embedding: torch.Tensor,\n",
    "    target_embedding: torch.Tensor,\n",
    "    n_heads: int,\n",
    "    causal_masking: bool = False,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Transformer decoder layer.\"\"\"\n",
    "    x = multi_head_attention(\n",
    "        target_embedding, target_embedding, target_embedding, n_heads\n",
    "    )\n",
    "    x = F.layer_norm(x + target_embedding, x.shape)\n",
    "    x = x + multi_head_attention(src_embedding, src_embedding, x, n_heads)\n",
    "    x = F.layer_norm(x, x.shape)\n",
    "\n",
    "    linear_1 = nn.Linear(EMBEDDING_DIM, 2 * EMBEDDING_DIM)\n",
    "    linear_2 = nn.Linear(2 * EMBEDDING_DIM, EMBEDDING_DIM)\n",
    "\n",
    "    x = x + F.relu(linear_2(linear_1(x)))\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "decoder_output = transformer_decoder_layer(embedded_tokens, embedded_tokens, n_heads=2)\n",
    "decoder_output.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where to go from Here\n",
    "\n",
    "Now that we have a basic insight into attention and transformers we will be using PyTorch's `torch.nn.TransformerEncoderLayer` and `torch.nn.TransformerDecoderLayer` modules in subsequent notebooks, to compose and train transformer-based models for tackling NLP tasks (e.g., text generation)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful Resources\n",
    "\n",
    "1. [Introduction to PyTorch](https://alexioannides.com/data-science-and-ml-notebook/pytorch/)\n",
    "2. [PyTorch docs](https://pytorch.org/docs/stable/index.html)\n",
    "3. [The Annotated Transformer](http://nlp.seas.harvard.edu/annotated-transformer/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
